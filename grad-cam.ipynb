{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10, 3, 481, 793])\n",
      "data_ready\n"
     ]
    }
   ],
   "source": [
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "ys_0=list()\n",
    "data_number= 4\n",
    "batch_size = 2\n",
    "epochs = 10000\n",
    "#100장정도.. 램 가능\n",
    "with open ('data/Data_narrow.txt', 'rt' ) as r_n:\n",
    "    target_r = r_n.readline()\n",
    "    for j in range(data_number): \n",
    "        #print(data_number)\n",
    "        xs_0=list()\n",
    "        for i in range(10): \n",
    "            temp_red=list()\n",
    "            temp_blue=list()\n",
    "            temp_green=list()\n",
    "            \n",
    "            r=open ('data/'+str(i+10*j)+'_building_r.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_red.append(target)\n",
    "                \n",
    "            r=open ('data/'+str(i+10*j)+'_building_g.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_green.append(target)\n",
    "                \n",
    "            r=open ('data/'+str(i+10*j)+'_building_b.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_blue.append(target)\n",
    "                \n",
    "            temp_red=np.array(temp_red, dtype=np.float32)\n",
    "            temp_greed=np.array(temp_green, dtype=np.float32)\n",
    "            temp_blue=np.array(temp_blue, dtype=np.float32)\n",
    "\n",
    "            xs_1=np.stack((temp_red, temp_green, temp_blue), axis=0)\n",
    "            xs_1=list(xs_1) \n",
    "            xs_0.append(xs_1)\n",
    "            \n",
    "            target_r = r_n.readline()\n",
    "            if i<10:\n",
    "                A_r,B_r,C_r,D_r,E_r,F_r,G_r,H_r,I_r,J_r,K_r=target_r.split(',')  \n",
    "\n",
    "                if i==0:\n",
    "                    pass\n",
    "\n",
    "                if i==1:\n",
    "                    ys_0.append([float(E_r)])\n",
    "\n",
    "                if i>=2 and i<10:\n",
    "\n",
    "                    ys_0=ys_0[:-1]+[ys_0[-1]+[float(E_r)]]\n",
    "\n",
    "        xs_0_.append(xs_0)\n",
    "\n",
    "x_train = np.array(xs_0_, dtype=np.float32)\n",
    "y_train = np.array(ys_0, dtype=np.float32)\n",
    "x_train = torch.from_numpy(x_train).cuda()\n",
    "y_train = torch.from_numpy(y_train).cuda()\n",
    "\n",
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "for j in range(data_number): \n",
    "    xs_0=list()\n",
    "    for i in range(10): \n",
    "        temp_red=list()\n",
    "        temp_blue=list()\n",
    "        temp_green=list()\n",
    "\n",
    "        r=open ('data/'+str(i+10*j)+'_r.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_red.append(target)\n",
    "\n",
    "        r=open ('data/'+str(i+10*j)+'_g.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_green.append(target)\n",
    "\n",
    "        r=open ('data/'+str(i+10*j)+'_b.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_blue.append(target)\n",
    "\n",
    "        temp_red=np.array(temp_red, dtype=np.float32)\n",
    "        temp_greed=np.array(temp_green, dtype=np.float32)\n",
    "        temp_blue=np.array(temp_blue, dtype=np.float32)\n",
    "\n",
    "        xs_1=np.stack((temp_red, temp_green, temp_blue), axis=0)\n",
    "        xs_1=list(xs_1) \n",
    "        xs_0.append(xs_1)\n",
    "    \n",
    "    xs_0_.append(xs_0)\n",
    "        \n",
    "x_test = np.array(xs_0_, dtype=np.float32)\n",
    "x_test = torch.from_numpy(x_test).cuda()\n",
    "#print(x_train.shape,y_train.shape)\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_train)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "print(x_test.shape)\n",
    "\n",
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "ys_0=list()\n",
    "temp_red=list()\n",
    "temp_blue=list()\n",
    "temp_green=list()\n",
    "\n",
    "print(\"data_ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.883407 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.860901 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.838424 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.815918 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.793486 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.771049 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.748653 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.726244 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.703905 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.681537 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.659210 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.636911 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.614605 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.592361 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.570113 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.547883 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.525667 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.503491 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.481308 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.459167 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.437031 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.414992 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.392952 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.370921 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.348912 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.326925 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.304906 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.282965 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.260971 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.239034 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.217100 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.195154 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.173243 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.151355 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.129478 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.107603 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.085716 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.063842 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.042006 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 21.020156 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.998323 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.976512 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.954652 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.932858 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.911052 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.889139 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.867210 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.845358 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.823708 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.802097 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.780461 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.758848 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.737245 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.715627 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.694026 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.672433 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.650826 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.629252 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.607648 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.586045 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.564457 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.542864 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.521274 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.499679 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.478064 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.456476 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.434911 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.413304 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.391718 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.370074 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.348473 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.326878 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.305370 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.283916 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.262457 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.241030 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.219543 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.198064 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.176571 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.155121 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.133653 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.112143 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.090632 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.069114 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.047613 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.026048 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 20.004551 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.982977 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.961412 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.939856 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.918270 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.896667 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.875075 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.853474 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.831848 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.810254 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.788548 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.766936 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.745226 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.723578 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.701860 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.680126 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.658411 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.636693 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.614923 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.593159 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.571349 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.549537 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.527686 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.505830 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.484006 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.462103 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 19.440214 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.418303 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.396348 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.374426 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.352456 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.330458 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.308442 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.286469 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.264383 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.242282 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.220191 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.198057 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.175884 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.153788 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.131556 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.109301 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.087042 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.064856 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.042472 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 19.020149 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.997770 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.975402 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.952952 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.930533 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.908045 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.885514 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.862982 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.840413 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.817818 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.795112 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.772492 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.749789 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.727024 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.704199 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.681480 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.658599 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.635738 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.612779 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.589823 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.566936 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.543858 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.520839 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.497689 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.474598 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.451408 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.428130 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.404884 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.381568 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.358315 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.334856 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.311524 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.287951 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.264430 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.240905 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.217337 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.193648 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.169954 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.146161 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.122429 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.098546 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.074614 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.050638 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.026630 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 18.002580 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.978477 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.954321 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.930143 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.905856 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.881473 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.857017 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.832600 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.807993 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.783331 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.758656 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.733909 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.709103 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.684277 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.659368 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.634306 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.609228 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.583957 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.558702 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.533381 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.508004 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.482497 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.457002 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.431284 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.405679 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.379892 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.354145 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.328165 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.302286 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.276243 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.250119 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.223888 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.197700 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.171362 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.144968 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.118474 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.091850 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.065225 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.038482 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 17.011610 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.984570 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.957467 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.930388 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.903207 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.875866 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.848447 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.820882 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.793242 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.765563 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 16.737843 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.709912 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.681950 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.653921 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.625779 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.597556 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.569171 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.540681 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.512135 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.483565 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.454943 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.425902 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.397110 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.367980 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.338871 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.309578 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.280175 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.250700 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.221125 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.191344 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.161583 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.131694 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.101554 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.071553 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.041172 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 16.010864 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.980349 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.949819 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.919075 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.888132 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.857196 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.826037 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.794930 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.763577 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.732040 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.700465 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.668892 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.636892 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.604939 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.572823 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.540505 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.508021 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.475549 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.442868 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.410161 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.377274 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.344292 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.310992 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.277529 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.244196 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.210546 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.176710 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.142737 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.108792 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.074312 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.040270 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 15.005735 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.970955 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.936104 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.901316 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.866132 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.830687 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.795382 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.759796 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.723945 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.687919 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.651939 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.615673 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.579163 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.542655 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.505874 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.468949 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.432010 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.394650 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.357305 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.319640 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.281952 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.243982 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.205771 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.167353 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.129002 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.090286 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.051518 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 14.012496 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.973414 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.933945 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.894401 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.854712 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.814693 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.774714 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.734224 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.693959 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.653137 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.612266 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.571389 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.529952 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.488723 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.446716 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.404951 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.363101 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.320709 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.278270 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.235940 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.193037 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.149972 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.106564 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.063138 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 13.019436 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.975805 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.931490 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.887115 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.842657 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 12.798136 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.753357 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.708014 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.662686 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.617029 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.571401 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.525475 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.479484 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.433064 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.386647 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.339595 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.292792 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.245555 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.198013 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.150337 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.102483 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.054403 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 12.006162 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.957712 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.908745 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.859835 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.810807 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.761580 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.711762 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.661924 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.611917 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.561604 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.511166 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.460432 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.409708 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.358660 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.307339 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.255749 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.204262 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.152117 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.100068 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 11.047819 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.994732 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.942200 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.889297 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.835979 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.782813 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.728805 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.675440 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.621434 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.567122 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.512844 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.458032 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.403538 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.348453 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.293121 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.237915 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.182248 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.126138 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.070378 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 10.014394 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.958115 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.901391 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.844884 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.787776 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.731096 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.674282 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.616278 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.558753 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.500921 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.443158 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.385386 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.326848 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.268646 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.210309 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.151512 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.092739 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 9.033817 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.974555 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.915655 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.856194 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.796910 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.737267 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.677358 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.617632 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.557665 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.497732 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.437079 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.376767 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.316417 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.256042 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.195646 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.134856 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.074191 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 8.013483 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.952475 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.891959 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.830866 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.769693 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.709038 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.647608 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.586046 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.524857 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.463440 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.402328 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.341197 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.279752 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.218438 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.156970 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.096312 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 7.034742 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.973499 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.912313 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.851323 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.789860 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 6.728679 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.667498 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.606853 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.545611 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.485043 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.424069 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.363323 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.302881 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.242326 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.182226 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.121523 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.061294 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 6.001026 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.941302 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.881649 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.821516 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.762002 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.703096 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.643670 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.584712 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.525863 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.467666 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.409108 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.350849 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.293087 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.235304 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.178047 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.120268 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.063206 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 5.006138 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.950362 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.893481 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.837731 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.781853 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.726563 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.670830 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.616156 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.561823 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.507579 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.453805 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.399726 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.346761 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.294127 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.240855 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.189203 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.136956 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.085836 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 4.034462 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.983860 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.933428 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.883273 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.833351 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.784819 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.735190 \n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.686680 \n",
      "\n",
      "Epoch 502\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.639324 \n",
      "\n",
      "Epoch 503\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.591502 \n",
      "\n",
      "Epoch 504\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.544140 \n",
      "\n",
      "Epoch 505\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.497726 \n",
      "\n",
      "Epoch 506\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.450871 \n",
      "\n",
      "Epoch 507\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.405541 \n",
      "\n",
      "Epoch 508\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.359311 \n",
      "\n",
      "Epoch 509\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.314334 \n",
      "\n",
      "Epoch 510\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.270412 \n",
      "\n",
      "Epoch 511\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.226033 \n",
      "\n",
      "Epoch 512\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.181865 \n",
      "\n",
      "Epoch 513\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.138731 \n",
      "\n",
      "Epoch 514\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.096025 \n",
      "\n",
      "Epoch 515\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.053785 \n",
      "\n",
      "Epoch 516\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 3.011930 \n",
      "\n",
      "Epoch 517\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.970568 \n",
      "\n",
      "Epoch 518\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.929005 \n",
      "\n",
      "Epoch 519\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.888168 \n",
      "\n",
      "Epoch 520\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.848676 \n",
      "\n",
      "Epoch 521\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.808564 \n",
      "\n",
      "Epoch 522\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.770088 \n",
      "\n",
      "Epoch 523\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.731154 \n",
      "\n",
      "Epoch 524\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.692916 \n",
      "\n",
      "Epoch 525\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.655563 \n",
      "\n",
      "Epoch 526\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.618051 \n",
      "\n",
      "Epoch 527\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.580853 \n",
      "\n",
      "Epoch 528\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.544344 \n",
      "\n",
      "Epoch 529\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.509008 \n",
      "\n",
      "Epoch 530\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.473844 \n",
      "\n",
      "Epoch 531\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.438389 \n",
      "\n",
      "Epoch 532\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.404054 \n",
      "\n",
      "Epoch 533\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.369893 \n",
      "\n",
      "Epoch 534\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.336513 \n",
      "\n",
      "Epoch 535\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.302934 \n",
      "\n",
      "Epoch 536\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.270356 \n",
      "\n",
      "Epoch 537\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.238283 \n",
      "\n",
      "Epoch 538\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.206731 \n",
      "\n",
      "Epoch 539\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.175178 \n",
      "\n",
      "Epoch 540\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.144647 \n",
      "\n",
      "Epoch 541\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.114404 \n",
      "\n",
      "Epoch 542\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.084662 \n",
      "\n",
      "Epoch 543\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.054941 \n",
      "\n",
      "Epoch 544\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 2.026131 \n",
      "\n",
      "Epoch 545\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.997415 \n",
      "\n",
      "Epoch 546\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.969321 \n",
      "\n",
      "Epoch 547\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.942023 \n",
      "\n",
      "Epoch 548\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.914398 \n",
      "\n",
      "Epoch 549\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.887678 \n",
      "\n",
      "Epoch 550\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.861205 \n",
      "\n",
      "Epoch 551\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.835052 \n",
      "\n",
      "Epoch 552\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.809422 \n",
      "\n",
      "Epoch 553\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.784532 \n",
      "\n",
      "Epoch 554\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.759705 \n",
      "\n",
      "Epoch 555\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.735510 \n",
      "\n",
      "Epoch 556\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.711853 \n",
      "\n",
      "Epoch 557\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.688338 \n",
      "\n",
      "Epoch 558\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 1.665351 \n",
      "\n",
      "Epoch 559\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.642658 \n",
      "\n",
      "Epoch 560\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.620645 \n",
      "\n",
      "Epoch 561\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.598502 \n",
      "\n",
      "Epoch 562\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.577353 \n",
      "\n",
      "Epoch 563\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.555659 \n",
      "\n",
      "Epoch 564\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.534881 \n",
      "\n",
      "Epoch 565\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.514748 \n",
      "\n",
      "Epoch 566\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.494823 \n",
      "\n",
      "Epoch 567\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.475098 \n",
      "\n",
      "Epoch 568\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.455958 \n",
      "\n",
      "Epoch 569\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.436954 \n",
      "\n",
      "Epoch 570\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.418798 \n",
      "\n",
      "Epoch 571\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.400378 \n",
      "\n",
      "Epoch 572\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.382423 \n",
      "\n",
      "Epoch 573\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.364928 \n",
      "\n",
      "Epoch 574\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.347683 \n",
      "\n",
      "Epoch 575\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.331451 \n",
      "\n",
      "Epoch 576\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.314341 \n",
      "\n",
      "Epoch 577\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.298241 \n",
      "\n",
      "Epoch 578\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.282576 \n",
      "\n",
      "Epoch 579\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.266588 \n",
      "\n",
      "Epoch 580\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.251341 \n",
      "\n",
      "Epoch 581\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.236508 \n",
      "\n",
      "Epoch 582\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.221736 \n",
      "\n",
      "Epoch 583\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.207723 \n",
      "\n",
      "Epoch 584\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.193575 \n",
      "\n",
      "Epoch 585\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.179715 \n",
      "\n",
      "Epoch 586\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.166268 \n",
      "\n",
      "Epoch 587\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.153078 \n",
      "\n",
      "Epoch 588\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.139725 \n",
      "\n",
      "Epoch 589\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.127278 \n",
      "\n",
      "Epoch 590\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.114678 \n",
      "\n",
      "Epoch 591\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.102522 \n",
      "\n",
      "Epoch 592\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.090624 \n",
      "\n",
      "Epoch 593\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.079085 \n",
      "\n",
      "Epoch 594\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.067598 \n",
      "\n",
      "Epoch 595\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.056806 \n",
      "\n",
      "Epoch 596\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.045616 \n",
      "\n",
      "Epoch 597\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.034718 \n",
      "\n",
      "Epoch 598\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.024407 \n",
      "\n",
      "Epoch 599\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.013969 \n",
      "\n",
      "Epoch 600\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 1.003913 \n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.994374 \n",
      "\n",
      "Epoch 602\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.984718 \n",
      "\n",
      "Epoch 603\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.975228 \n",
      "\n",
      "Epoch 604\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.966159 \n",
      "\n",
      "Epoch 605\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.957047 \n",
      "\n",
      "Epoch 606\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.948423 \n",
      "\n",
      "Epoch 607\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.939442 \n",
      "\n",
      "Epoch 608\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.931349 \n",
      "\n",
      "Epoch 609\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.923398 \n",
      "\n",
      "Epoch 610\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.914865 \n",
      "\n",
      "Epoch 611\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.907491 \n",
      "\n",
      "Epoch 612\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.899811 \n",
      "\n",
      "Epoch 613\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.892319 \n",
      "\n",
      "Epoch 614\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.884768 \n",
      "\n",
      "Epoch 615\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.877657 \n",
      "\n",
      "Epoch 616\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.870578 \n",
      "\n",
      "Epoch 617\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.863820 \n",
      "\n",
      "Epoch 618\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.857105 \n",
      "\n",
      "Epoch 619\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.850570 \n",
      "\n",
      "Epoch 620\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.844087 \n",
      "\n",
      "Epoch 621\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.838001 \n",
      "\n",
      "Epoch 622\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.831772 \n",
      "\n",
      "Epoch 623\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.825887 \n",
      "\n",
      "Epoch 624\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.820241 \n",
      "\n",
      "Epoch 625\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.814616 \n",
      "\n",
      "Epoch 626\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.808981 \n",
      "\n",
      "Epoch 627\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.803597 \n",
      "\n",
      "Epoch 628\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.798514 \n",
      "\n",
      "Epoch 629\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.793154 \n",
      "\n",
      "Epoch 630\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.788100 \n",
      "\n",
      "Epoch 631\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.783219 \n",
      "\n",
      "Epoch 632\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.778438 \n",
      "\n",
      "Epoch 633\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.773607 \n",
      "\n",
      "Epoch 634\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.769067 \n",
      "\n",
      "Epoch 635\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.764570 \n",
      "\n",
      "Epoch 636\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.760231 \n",
      "\n",
      "Epoch 637\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.756240 \n",
      "\n",
      "Epoch 638\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.752096 \n",
      "\n",
      "Epoch 639\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.747889 \n",
      "\n",
      "Epoch 640\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.743839 \n",
      "\n",
      "Epoch 641\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.740253 \n",
      "\n",
      "Epoch 642\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.736427 \n",
      "\n",
      "Epoch 643\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.732698 \n",
      "\n",
      "Epoch 644\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.729229 \n",
      "\n",
      "Epoch 645\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.725678 \n",
      "\n",
      "Epoch 646\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.722175 \n",
      "\n",
      "Epoch 647\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.718859 \n",
      "\n",
      "Epoch 648\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.715575 \n",
      "\n",
      "Epoch 649\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.712423 \n",
      "\n",
      "Epoch 650\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.709286 \n",
      "\n",
      "Epoch 651\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.706160 \n",
      "\n",
      "Epoch 652\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.703324 \n",
      "\n",
      "Epoch 653\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.700328 \n",
      "\n",
      "Epoch 654\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.697791 \n",
      "\n",
      "Epoch 655\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.695265 \n",
      "\n",
      "Epoch 656\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.692599 \n",
      "\n",
      "Epoch 657\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.689821 \n",
      "\n",
      "Epoch 658\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.687135 \n",
      "\n",
      "Epoch 659\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.684681 \n",
      "\n",
      "Epoch 660\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.682346 \n",
      "\n",
      "Epoch 661\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.680018 \n",
      "\n",
      "Epoch 662\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.677628 \n",
      "\n",
      "Epoch 663\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.675490 \n",
      "\n",
      "Epoch 664\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.673324 \n",
      "\n",
      "Epoch 665\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.671338 \n",
      "\n",
      "Epoch 666\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.669105 \n",
      "\n",
      "Epoch 667\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.667047 \n",
      "\n",
      "Epoch 668\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.665005 \n",
      "\n",
      "Epoch 669\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.663417 \n",
      "\n",
      "Epoch 670\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.661304 \n",
      "\n",
      "Epoch 671\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.659545 \n",
      "\n",
      "Epoch 672\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.657611 \n",
      "\n",
      "Epoch 673\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.656063 \n",
      "\n",
      "Epoch 674\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.654167 \n",
      "\n",
      "Epoch 675\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.652714 \n",
      "\n",
      "Epoch 676\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.651195 \n",
      "\n",
      "Epoch 677\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.649515 \n",
      "\n",
      "Epoch 678\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.647980 \n",
      "\n",
      "Epoch 679\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.646421 \n",
      "\n",
      "Epoch 680\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.645032 \n",
      "\n",
      "Epoch 681\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.643798 \n",
      "\n",
      "Epoch 682\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.642137 \n",
      "\n",
      "Epoch 683\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.641011 \n",
      "\n",
      "Epoch 684\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.639633 \n",
      "\n",
      "Epoch 685\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.638229 \n",
      "\n",
      "Epoch 686\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.637162 \n",
      "\n",
      "Epoch 687\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.635817 \n",
      "\n",
      "Epoch 688\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.634752 \n",
      "\n",
      "Epoch 689\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.633486 \n",
      "\n",
      "Epoch 690\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.632371 \n",
      "\n",
      "Epoch 691\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.631264 \n",
      "\n",
      "Epoch 692\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.630237 \n",
      "\n",
      "Epoch 693\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.629238 \n",
      "\n",
      "Epoch 694\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.628144 \n",
      "\n",
      "Epoch 695\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.627302 \n",
      "\n",
      "Epoch 696\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.626241 \n",
      "\n",
      "Epoch 697\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.625306 \n",
      "\n",
      "Epoch 698\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.624345 \n",
      "\n",
      "Epoch 699\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.623492 \n",
      "\n",
      "Epoch 700\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.622645 \n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.621739 \n",
      "\n",
      "Epoch 702\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.620884 \n",
      "\n",
      "Epoch 703\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.620236 \n",
      "\n",
      "Epoch 704\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.619352 \n",
      "\n",
      "Epoch 705\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.618665 \n",
      "\n",
      "Epoch 706\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.617991 \n",
      "\n",
      "Epoch 707\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.617141 \n",
      "\n",
      "Epoch 708\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.616449 \n",
      "\n",
      "Epoch 709\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.615739 \n",
      "\n",
      "Epoch 710\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.615220 \n",
      "\n",
      "Epoch 711\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.614452 \n",
      "\n",
      "Epoch 712\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.613865 \n",
      "\n",
      "Epoch 713\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.613210 \n",
      "\n",
      "Epoch 714\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.612608 \n",
      "\n",
      "Epoch 715\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.612080 \n",
      "\n",
      "Epoch 716\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.611361 \n",
      "\n",
      "Epoch 717\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.610878 \n",
      "\n",
      "Epoch 718\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.610395 \n",
      "\n",
      "Epoch 719\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.609816 \n",
      "\n",
      "Epoch 720\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.609362 \n",
      "\n",
      "Epoch 721\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.608764 \n",
      "\n",
      "Epoch 722\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.608268 \n",
      "\n",
      "Epoch 723\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.607889 \n",
      "\n",
      "Epoch 724\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.607415 \n",
      "\n",
      "Epoch 725\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.606886 \n",
      "\n",
      "Epoch 726\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.606493 \n",
      "\n",
      "Epoch 727\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.606083 \n",
      "\n",
      "Epoch 728\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.605672 \n",
      "\n",
      "Epoch 729\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.605217 \n",
      "\n",
      "Epoch 730\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.604884 \n",
      "\n",
      "Epoch 731\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.604428 \n",
      "\n",
      "Epoch 732\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.603991 \n",
      "\n",
      "Epoch 733\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.603709 \n",
      "\n",
      "Epoch 734\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.603362 \n",
      "\n",
      "Epoch 735\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.602892 \n",
      "\n",
      "Epoch 736\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.602714 \n",
      "\n",
      "Epoch 737\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.602378 \n",
      "\n",
      "Epoch 738\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.601825 \n",
      "\n",
      "Epoch 739\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.601653 \n",
      "\n",
      "Epoch 740\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.601487 \n",
      "\n",
      "Epoch 741\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.601049 \n",
      "\n",
      "Epoch 742\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.600686 \n",
      "\n",
      "Epoch 743\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.600479 \n",
      "\n",
      "Epoch 744\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.600284 \n",
      "\n",
      "Epoch 745\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.599832 \n",
      "\n",
      "Epoch 746\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.599586 \n",
      "\n",
      "Epoch 747\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.599442 \n",
      "\n",
      "Epoch 748\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.599190 \n",
      "\n",
      "Epoch 749\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.598866 \n",
      "\n",
      "Epoch 750\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.598541 \n",
      "\n",
      "Epoch 751\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.598562 \n",
      "\n",
      "Epoch 752\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.598274 \n",
      "\n",
      "Epoch 753\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.597995 \n",
      "\n",
      "Epoch 754\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.597598 \n",
      "\n",
      "Epoch 755\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.597505 \n",
      "\n",
      "Epoch 756\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.597400 \n",
      "\n",
      "Epoch 757\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.597207 \n",
      "\n",
      "Epoch 758\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.596889 \n",
      "\n",
      "Epoch 759\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.596714 \n",
      "\n",
      "Epoch 760\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.596548 \n",
      "\n",
      "Epoch 761\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.596379 \n",
      "\n",
      "Epoch 762\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.596177 \n",
      "\n",
      "Epoch 763\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.596009 \n",
      "\n",
      "Epoch 764\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.595859 \n",
      "\n",
      "Epoch 765\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.595776 \n",
      "\n",
      "Epoch 766\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.595501 \n",
      "\n",
      "Epoch 767\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.595346 \n",
      "\n",
      "Epoch 768\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.595143 \n",
      "\n",
      "Epoch 769\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.595080 \n",
      "\n",
      "Epoch 770\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.594976 \n",
      "\n",
      "Epoch 771\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.594775 \n",
      "\n",
      "Epoch 772\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.594719 \n",
      "\n",
      "Epoch 773\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.594516 \n",
      "\n",
      "Epoch 774\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.594403 \n",
      "\n",
      "Epoch 775\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.594232 \n",
      "\n",
      "Epoch 776\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.594095 \n",
      "\n",
      "Epoch 777\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593975 \n",
      "\n",
      "Epoch 778\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593919 \n",
      "\n",
      "Epoch 779\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593677 \n",
      "\n",
      "Epoch 780\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593665 \n",
      "\n",
      "Epoch 781\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593499 \n",
      "\n",
      "Epoch 782\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.593405 \n",
      "\n",
      "Epoch 783\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593329 \n",
      "\n",
      "Epoch 784\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593251 \n",
      "\n",
      "Epoch 785\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.593033 \n",
      "\n",
      "Epoch 786\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592967 \n",
      "\n",
      "Epoch 787\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592913 \n",
      "\n",
      "Epoch 788\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592786 \n",
      "\n",
      "Epoch 789\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592778 \n",
      "\n",
      "Epoch 790\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592697 \n",
      "\n",
      "Epoch 791\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592642 \n",
      "\n",
      "Epoch 792\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592489 \n",
      "\n",
      "Epoch 793\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592361 \n",
      "\n",
      "Epoch 794\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592238 \n",
      "\n",
      "Epoch 795\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592218 \n",
      "\n",
      "Epoch 796\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592152 \n",
      "\n",
      "Epoch 797\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592058 \n",
      "\n",
      "Epoch 798\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.592075 \n",
      "\n",
      "Epoch 799\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591901 \n",
      "\n",
      "Epoch 800\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591817 \n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591767 \n",
      "\n",
      "Epoch 802\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591672 \n",
      "\n",
      "Epoch 803\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591716 \n",
      "\n",
      "Epoch 804\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591528 \n",
      "\n",
      "Epoch 805\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591480 \n",
      "\n",
      "Epoch 806\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591384 \n",
      "\n",
      "Epoch 807\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591343 \n",
      "\n",
      "Epoch 808\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591244 \n",
      "\n",
      "Epoch 809\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591225 \n",
      "\n",
      "Epoch 810\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591205 \n",
      "\n",
      "Epoch 811\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591220 \n",
      "\n",
      "Epoch 812\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591188 \n",
      "\n",
      "Epoch 813\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591079 \n",
      "\n",
      "Epoch 814\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590986 \n",
      "\n",
      "Epoch 815\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.591022 \n",
      "\n",
      "Epoch 816\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590928 \n",
      "\n",
      "Epoch 817\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590866 \n",
      "\n",
      "Epoch 818\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590738 \n",
      "\n",
      "Epoch 819\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590771 \n",
      "\n",
      "Epoch 820\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590819 \n",
      "\n",
      "Epoch 821\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590799 \n",
      "\n",
      "Epoch 822\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590719 \n",
      "\n",
      "Epoch 823\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590603 \n",
      "\n",
      "Epoch 824\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590537 \n",
      "\n",
      "Epoch 825\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590514 \n",
      "\n",
      "Epoch 826\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590444 \n",
      "\n",
      "Epoch 827\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590446 \n",
      "\n",
      "Epoch 828\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590372 \n",
      "\n",
      "Epoch 829\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590344 \n",
      "\n",
      "Epoch 830\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590298 \n",
      "\n",
      "Epoch 831\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590298 \n",
      "\n",
      "Epoch 832\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590396 \n",
      "\n",
      "Epoch 833\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590209 \n",
      "\n",
      "Epoch 834\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590186 \n",
      "\n",
      "Epoch 835\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590229 \n",
      "\n",
      "Epoch 836\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590211 \n",
      "\n",
      "Epoch 837\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590155 \n",
      "\n",
      "Epoch 838\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590141 \n",
      "\n",
      "Epoch 839\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590066 \n",
      "\n",
      "Epoch 840\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590050 \n",
      "\n",
      "Epoch 841\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.590061 \n",
      "\n",
      "Epoch 842\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589951 \n",
      "\n",
      "Epoch 843\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589858 \n",
      "\n",
      "Epoch 844\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589785 \n",
      "\n",
      "Epoch 845\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589819 \n",
      "\n",
      "Epoch 846\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589818 \n",
      "\n",
      "Epoch 847\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589817 \n",
      "\n",
      "Epoch 848\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589711 \n",
      "\n",
      "Epoch 849\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589783 \n",
      "\n",
      "Epoch 850\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589825 \n",
      "\n",
      "Epoch 851\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589802 \n",
      "\n",
      "Epoch 852\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589775 \n",
      "\n",
      "Epoch 853\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589857 \n",
      "\n",
      "Epoch 854\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589788 \n",
      "\n",
      "Epoch 855\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589809 \n",
      "\n",
      "Epoch 856\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589792 \n",
      "\n",
      "Epoch 857\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589741 \n",
      "\n",
      "Epoch 858\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589628 \n",
      "\n",
      "Epoch 859\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589580 \n",
      "\n",
      "Epoch 860\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589454 \n",
      "\n",
      "Epoch 861\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589420 \n",
      "\n",
      "Epoch 862\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589381 \n",
      "\n",
      "Epoch 863\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589391 \n",
      "\n",
      "Epoch 864\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589404 \n",
      "\n",
      "Epoch 865\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589420 \n",
      "\n",
      "Epoch 866\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589422 \n",
      "\n",
      "Epoch 867\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589411 \n",
      "\n",
      "Epoch 868\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589448 \n",
      "\n",
      "Epoch 869\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589438 \n",
      "\n",
      "Epoch 870\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589489 \n",
      "\n",
      "Epoch 871\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589459 \n",
      "\n",
      "Epoch 872\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589472 \n",
      "\n",
      "Epoch 873\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589481 \n",
      "\n",
      "Epoch 874\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589496 \n",
      "\n",
      "Epoch 875\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589451 \n",
      "\n",
      "Epoch 876\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589427 \n",
      "\n",
      "Epoch 877\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589414 \n",
      "\n",
      "Epoch 878\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589300 \n",
      "\n",
      "Epoch 879\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589272 \n",
      "\n",
      "Epoch 880\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589281 \n",
      "\n",
      "Epoch 881\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589245 \n",
      "\n",
      "Epoch 882\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589132 \n",
      "\n",
      "Epoch 883\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589079 \n",
      "\n",
      "Epoch 884\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589092 \n",
      "\n",
      "Epoch 885\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589097 \n",
      "\n",
      "Epoch 886\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589062 \n",
      "\n",
      "Epoch 887\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589077 \n",
      "\n",
      "Epoch 888\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589015 \n",
      "\n",
      "Epoch 889\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589012 \n",
      "\n",
      "Epoch 890\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588998 \n",
      "\n",
      "Epoch 891\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589038 \n",
      "\n",
      "Epoch 892\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589085 \n",
      "\n",
      "Epoch 893\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589094 \n",
      "\n",
      "Epoch 894\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.589070 \n",
      "\n",
      "Epoch 895\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589065 \n",
      "\n",
      "Epoch 896\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589035 \n",
      "\n",
      "Epoch 897\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589055 \n",
      "\n",
      "Epoch 898\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588987 \n",
      "\n",
      "Epoch 899\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589039 \n",
      "\n",
      "Epoch 900\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589092 \n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589088 \n",
      "\n",
      "Epoch 902\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589099 \n",
      "\n",
      "Epoch 903\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589056 \n",
      "\n",
      "Epoch 904\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589088 \n",
      "\n",
      "Epoch 905\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589050 \n",
      "\n",
      "Epoch 906\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589012 \n",
      "\n",
      "Epoch 907\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589031 \n",
      "\n",
      "Epoch 908\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589038 \n",
      "\n",
      "Epoch 909\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.589019 \n",
      "\n",
      "Epoch 910\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588980 \n",
      "\n",
      "Epoch 911\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588926 \n",
      "\n",
      "Epoch 912\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588929 \n",
      "\n",
      "Epoch 913\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588898 \n",
      "\n",
      "Epoch 914\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588920 \n",
      "\n",
      "Epoch 915\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588870 \n",
      "\n",
      "Epoch 916\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588845 \n",
      "\n",
      "Epoch 917\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588881 \n",
      "\n",
      "Epoch 918\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588886 \n",
      "\n",
      "Epoch 919\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588885 \n",
      "\n",
      "Epoch 920\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588878 \n",
      "\n",
      "Epoch 921\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588870 \n",
      "\n",
      "Epoch 922\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588832 \n",
      "\n",
      "Epoch 923\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588835 \n",
      "\n",
      "Epoch 924\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588793 \n",
      "\n",
      "Epoch 925\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588757 \n",
      "\n",
      "Epoch 926\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588773 \n",
      "\n",
      "Epoch 927\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588760 \n",
      "\n",
      "Epoch 928\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588743 \n",
      "\n",
      "Epoch 929\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588754 \n",
      "\n",
      "Epoch 930\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588761 \n",
      "\n",
      "Epoch 931\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588725 \n",
      "\n",
      "Epoch 932\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588713 \n",
      "\n",
      "Epoch 933\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588736 \n",
      "\n",
      "Epoch 934\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588672 \n",
      "\n",
      "Epoch 935\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588737 \n",
      "\n",
      "Epoch 936\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588714 \n",
      "\n",
      "Epoch 937\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588697 \n",
      "\n",
      "Epoch 938\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588701 \n",
      "\n",
      "Epoch 939\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588687 \n",
      "\n",
      "Epoch 940\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588744 \n",
      "\n",
      "Epoch 941\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588698 \n",
      "\n",
      "Epoch 942\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588677 \n",
      "\n",
      "Epoch 943\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588652 \n",
      "\n",
      "Epoch 944\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588603 \n",
      "\n",
      "Epoch 945\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588627 \n",
      "\n",
      "Epoch 946\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588657 \n",
      "\n",
      "Epoch 947\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588660 \n",
      "\n",
      "Epoch 948\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588617 \n",
      "\n",
      "Epoch 949\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588615 \n",
      "\n",
      "Epoch 950\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588630 \n",
      "\n",
      "Epoch 951\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588626 \n",
      "\n",
      "Epoch 952\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588576 \n",
      "\n",
      "Epoch 953\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588534 \n",
      "\n",
      "Epoch 954\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588526 \n",
      "\n",
      "Epoch 955\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588554 \n",
      "\n",
      "Epoch 956\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588501 \n",
      "\n",
      "Epoch 957\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588521 \n",
      "\n",
      "Epoch 958\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588512 \n",
      "\n",
      "Epoch 959\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588515 \n",
      "\n",
      "Epoch 960\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588502 \n",
      "\n",
      "Epoch 961\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588517 \n",
      "\n",
      "Epoch 962\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588469 \n",
      "\n",
      "Epoch 963\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588448 \n",
      "\n",
      "Epoch 964\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588503 \n",
      "\n",
      "Epoch 965\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588544 \n",
      "\n",
      "Epoch 966\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588511 \n",
      "\n",
      "Epoch 967\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588480 \n",
      "\n",
      "Epoch 968\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588508 \n",
      "\n",
      "Epoch 969\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588520 \n",
      "\n",
      "Epoch 970\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588551 \n",
      "\n",
      "Epoch 971\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588555 \n",
      "\n",
      "Epoch 972\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588496 \n",
      "\n",
      "Epoch 973\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588468 \n",
      "\n",
      "Epoch 974\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588439 \n",
      "\n",
      "Epoch 975\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588442 \n",
      "\n",
      "Epoch 976\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588394 \n",
      "\n",
      "Epoch 977\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588389 \n",
      "\n",
      "Epoch 978\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588406 \n",
      "\n",
      "Epoch 979\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588331 \n",
      "\n",
      "Epoch 980\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588380 \n",
      "\n",
      "Epoch 981\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588361 \n",
      "\n",
      "Epoch 982\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588358 \n",
      "\n",
      "Epoch 983\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588382 \n",
      "\n",
      "Epoch 984\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588291 \n",
      "\n",
      "Epoch 985\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588262 \n",
      "\n",
      "Epoch 986\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588284 \n",
      "\n",
      "Epoch 987\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588276 \n",
      "\n",
      "Epoch 988\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588274 \n",
      "\n",
      "Epoch 989\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588284 \n",
      "\n",
      "Epoch 990\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588285 \n",
      "\n",
      "Epoch 991\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588263 \n",
      "\n",
      "Epoch 992\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588299 \n",
      "\n",
      "Epoch 993\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588289 \n",
      "\n",
      "Epoch 994\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588323 \n",
      "\n",
      "Epoch 995\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588348 \n",
      "\n",
      "Epoch 996\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588357 \n",
      "\n",
      "Epoch 997\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588353 \n",
      "\n",
      "Epoch 998\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588366 \n",
      "\n",
      "Epoch 999\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588353 \n",
      "\n",
      "Epoch 1000\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588358 \n",
      "\n",
      "Epoch 1001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588357 \n",
      "\n",
      "Epoch 1002\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588338 \n",
      "\n",
      "Epoch 1003\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588343 \n",
      "\n",
      "Epoch 1004\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588344 \n",
      "\n",
      "Epoch 1005\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.588335 \n",
      "\n",
      "Epoch 1006\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588351 \n",
      "\n",
      "Epoch 1007\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588348 \n",
      "\n",
      "Epoch 1008\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588368 \n",
      "\n",
      "Epoch 1009\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588356 \n",
      "\n",
      "Epoch 1010\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588347 \n",
      "\n",
      "Epoch 1011\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588344 \n",
      "\n",
      "Epoch 1012\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588351 \n",
      "\n",
      "Epoch 1013\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588338 \n",
      "\n",
      "Epoch 1014\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588333 \n",
      "\n",
      "Epoch 1015\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588330 \n",
      "\n",
      "Epoch 1016\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588339 \n",
      "\n",
      "Epoch 1017\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588286 \n",
      "\n",
      "Epoch 1018\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588295 \n",
      "\n",
      "Epoch 1019\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588299 \n",
      "\n",
      "Epoch 1020\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588284 \n",
      "\n",
      "Epoch 1021\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588294 \n",
      "\n",
      "Epoch 1022\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588253 \n",
      "\n",
      "Epoch 1023\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588291 \n",
      "\n",
      "Epoch 1024\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588215 \n",
      "\n",
      "Epoch 1025\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588235 \n",
      "\n",
      "Epoch 1026\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588295 \n",
      "\n",
      "Epoch 1027\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588209 \n",
      "\n",
      "Epoch 1028\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588211 \n",
      "\n",
      "Epoch 1029\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588232 \n",
      "\n",
      "Epoch 1030\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588213 \n",
      "\n",
      "Epoch 1031\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588223 \n",
      "\n",
      "Epoch 1032\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588157 \n",
      "\n",
      "Epoch 1033\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588185 \n",
      "\n",
      "Epoch 1034\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588226 \n",
      "\n",
      "Epoch 1035\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588138 \n",
      "\n",
      "Epoch 1036\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588157 \n",
      "\n",
      "Epoch 1037\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588141 \n",
      "\n",
      "Epoch 1038\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588145 \n",
      "\n",
      "Epoch 1039\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588162 \n",
      "\n",
      "Epoch 1040\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588160 \n",
      "\n",
      "Epoch 1041\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588136 \n",
      "\n",
      "Epoch 1042\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588127 \n",
      "\n",
      "Epoch 1043\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588146 \n",
      "\n",
      "Epoch 1044\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588146 \n",
      "\n",
      "Epoch 1045\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588106 \n",
      "\n",
      "Epoch 1046\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588108 \n",
      "\n",
      "Epoch 1047\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588122 \n",
      "\n",
      "Epoch 1048\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588108 \n",
      "\n",
      "Epoch 1049\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588105 \n",
      "\n",
      "Epoch 1050\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588107 \n",
      "\n",
      "Epoch 1051\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588082 \n",
      "\n",
      "Epoch 1052\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588068 \n",
      "\n",
      "Epoch 1053\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588084 \n",
      "\n",
      "Epoch 1054\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588078 \n",
      "\n",
      "Epoch 1055\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588071 \n",
      "\n",
      "Epoch 1056\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588078 \n",
      "\n",
      "Epoch 1057\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588066 \n",
      "\n",
      "Epoch 1058\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588091 \n",
      "\n",
      "Epoch 1059\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588094 \n",
      "\n",
      "Epoch 1060\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588062 \n",
      "\n",
      "Epoch 1061\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588066 \n",
      "\n",
      "Epoch 1062\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588065 \n",
      "\n",
      "Epoch 1063\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588044 \n",
      "\n",
      "Epoch 1064\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588054 \n",
      "\n",
      "Epoch 1065\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588073 \n",
      "\n",
      "Epoch 1066\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588054 \n",
      "\n",
      "Epoch 1067\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588075 \n",
      "\n",
      "Epoch 1068\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588061 \n",
      "\n",
      "Epoch 1069\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588060 \n",
      "\n",
      "Epoch 1070\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588083 \n",
      "\n",
      "Epoch 1071\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588057 \n",
      "\n",
      "Epoch 1072\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588058 \n",
      "\n",
      "Epoch 1073\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588047 \n",
      "\n",
      "Epoch 1074\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588051 \n",
      "\n",
      "Epoch 1075\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588043 \n",
      "\n",
      "Epoch 1076\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588065 \n",
      "\n",
      "Epoch 1077\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588045 \n",
      "\n",
      "Epoch 1078\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588026 \n",
      "\n",
      "Epoch 1079\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588021 \n",
      "\n",
      "Epoch 1080\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588024 \n",
      "\n",
      "Epoch 1081\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588023 \n",
      "\n",
      "Epoch 1082\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588020 \n",
      "\n",
      "Epoch 1083\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588052 \n",
      "\n",
      "Epoch 1084\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588027 \n",
      "\n",
      "Epoch 1085\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588021 \n",
      "\n",
      "Epoch 1086\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588018 \n",
      "\n",
      "Epoch 1087\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588004 \n",
      "\n",
      "Epoch 1088\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588012 \n",
      "\n",
      "Epoch 1089\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587981 \n",
      "\n",
      "Epoch 1090\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588003 \n",
      "\n",
      "Epoch 1091\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.588010 \n",
      "\n",
      "Epoch 1092\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587998 \n",
      "\n",
      "Epoch 1093\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587981 \n",
      "\n",
      "Epoch 1094\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587978 \n",
      "\n",
      "Epoch 1095\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587983 \n",
      "\n",
      "Epoch 1096\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587947 \n",
      "\n",
      "Epoch 1097\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587947 \n",
      "\n",
      "Epoch 1098\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587952 \n",
      "\n",
      "Epoch 1099\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587953 \n",
      "\n",
      "Epoch 1100\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587944 \n",
      "\n",
      "Epoch 1101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587945 \n",
      "\n",
      "Epoch 1102\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587941 \n",
      "\n",
      "Epoch 1103\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587940 \n",
      "\n",
      "Epoch 1104\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587932 \n",
      "\n",
      "Epoch 1105\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587927 \n",
      "\n",
      "Epoch 1106\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587928 \n",
      "\n",
      "Epoch 1107\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587927 \n",
      "\n",
      "Epoch 1108\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587928 \n",
      "\n",
      "Epoch 1109\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587929 \n",
      "\n",
      "Epoch 1110\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587929 \n",
      "\n",
      "Epoch 1111\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587928 \n",
      "\n",
      "Epoch 1112\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587929 \n",
      "\n",
      "Epoch 1113\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.587919 \n",
      "\n",
      "Epoch 1114\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587922 \n",
      "\n",
      "Epoch 1115\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587888 \n",
      "\n",
      "Epoch 1116\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587927 \n",
      "\n",
      "Epoch 1117\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587916 \n",
      "\n",
      "Epoch 1118\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587914 \n",
      "\n",
      "Epoch 1119\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587930 \n",
      "\n",
      "Epoch 1120\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587911 \n",
      "\n",
      "Epoch 1121\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587928 \n",
      "\n",
      "Epoch 1122\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587919 \n",
      "\n",
      "Epoch 1123\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587930 \n",
      "\n",
      "Epoch 1124\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587905 \n",
      "\n",
      "Epoch 1125\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587892 \n",
      "\n",
      "Epoch 1126\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587897 \n",
      "\n",
      "Epoch 1127\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587875 \n",
      "\n",
      "Epoch 1128\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587868 \n",
      "\n",
      "Epoch 1129\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587893 \n",
      "\n",
      "Epoch 1130\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587917 \n",
      "\n",
      "Epoch 1131\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587895 \n",
      "\n",
      "Epoch 1132\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587884 \n",
      "\n",
      "Epoch 1133\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587889 \n",
      "\n",
      "Epoch 1134\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587884 \n",
      "\n",
      "Epoch 1135\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587900 \n",
      "\n",
      "Epoch 1136\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587868 \n",
      "\n",
      "Epoch 1137\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587887 \n",
      "\n",
      "Epoch 1138\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587878 \n",
      "\n",
      "Epoch 1139\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587886 \n",
      "\n",
      "Epoch 1140\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587855 \n",
      "\n",
      "Epoch 1141\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587877 \n",
      "\n",
      "Epoch 1142\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587884 \n",
      "\n",
      "Epoch 1143\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587821 \n",
      "\n",
      "Epoch 1144\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587807 \n",
      "\n",
      "Epoch 1145\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587836 \n",
      "\n",
      "Epoch 1146\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587790 \n",
      "\n",
      "Epoch 1147\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587798 \n",
      "\n",
      "Epoch 1148\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587799 \n",
      "\n",
      "Epoch 1149\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587803 \n",
      "\n",
      "Epoch 1150\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587776 \n",
      "\n",
      "Epoch 1151\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587777 \n",
      "\n",
      "Epoch 1152\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587779 \n",
      "\n",
      "Epoch 1153\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587776 \n",
      "\n",
      "Epoch 1154\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587766 \n",
      "\n",
      "Epoch 1155\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587723 \n",
      "\n",
      "Epoch 1156\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587722 \n",
      "\n",
      "Epoch 1157\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587752 \n",
      "\n",
      "Epoch 1158\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587771 \n",
      "\n",
      "Epoch 1159\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587686 \n",
      "\n",
      "Epoch 1160\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587674 \n",
      "\n",
      "Epoch 1161\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587646 \n",
      "\n",
      "Epoch 1162\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587701 \n",
      "\n",
      "Epoch 1163\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587677 \n",
      "\n",
      "Epoch 1164\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587668 \n",
      "\n",
      "Epoch 1165\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587646 \n",
      "\n",
      "Epoch 1166\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587640 \n",
      "\n",
      "Epoch 1167\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587634 \n",
      "\n",
      "Epoch 1168\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587665 \n",
      "\n",
      "Epoch 1169\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587643 \n",
      "\n",
      "Epoch 1170\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587677 \n",
      "\n",
      "Epoch 1171\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587649 \n",
      "\n",
      "Epoch 1172\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587643 \n",
      "\n",
      "Epoch 1173\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587618 \n",
      "\n",
      "Epoch 1174\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587651 \n",
      "\n",
      "Epoch 1175\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587637 \n",
      "\n",
      "Epoch 1176\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587610 \n",
      "\n",
      "Epoch 1177\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587616 \n",
      "\n",
      "Epoch 1178\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587628 \n",
      "\n",
      "Epoch 1179\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587601 \n",
      "\n",
      "Epoch 1180\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587615 \n",
      "\n",
      "Epoch 1181\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587606 \n",
      "\n",
      "Epoch 1182\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587582 \n",
      "\n",
      "Epoch 1183\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587598 \n",
      "\n",
      "Epoch 1184\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587564 \n",
      "\n",
      "Epoch 1185\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587564 \n",
      "\n",
      "Epoch 1186\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587553 \n",
      "\n",
      "Epoch 1187\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587546 \n",
      "\n",
      "Epoch 1188\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587576 \n",
      "\n",
      "Epoch 1189\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587545 \n",
      "\n",
      "Epoch 1190\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587566 \n",
      "\n",
      "Epoch 1191\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587523 \n",
      "\n",
      "Epoch 1192\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587557 \n",
      "\n",
      "Epoch 1193\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587479 \n",
      "\n",
      "Epoch 1194\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587512 \n",
      "\n",
      "Epoch 1195\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587526 \n",
      "\n",
      "Epoch 1196\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587516 \n",
      "\n",
      "Epoch 1197\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587481 \n",
      "\n",
      "Epoch 1198\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587443 \n",
      "\n",
      "Epoch 1199\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587475 \n",
      "\n",
      "Epoch 1200\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587473 \n",
      "\n",
      "Epoch 1201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587442 \n",
      "\n",
      "Epoch 1202\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587428 \n",
      "\n",
      "Epoch 1203\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587451 \n",
      "\n",
      "Epoch 1204\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587451 \n",
      "\n",
      "Epoch 1205\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587426 \n",
      "\n",
      "Epoch 1206\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587482 \n",
      "\n",
      "Epoch 1207\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587420 \n",
      "\n",
      "Epoch 1208\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587417 \n",
      "\n",
      "Epoch 1209\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587417 \n",
      "\n",
      "Epoch 1210\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587422 \n",
      "\n",
      "Epoch 1211\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587442 \n",
      "\n",
      "Epoch 1212\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587466 \n",
      "\n",
      "Epoch 1213\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587432 \n",
      "\n",
      "Epoch 1214\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587407 \n",
      "\n",
      "Epoch 1215\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587323 \n",
      "\n",
      "Epoch 1216\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587322 \n",
      "\n",
      "Epoch 1217\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587421 \n",
      "\n",
      "Epoch 1218\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587410 \n",
      "\n",
      "Epoch 1219\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587327 \n",
      "\n",
      "Epoch 1220\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587384 \n",
      "\n",
      "Epoch 1221\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.587379 \n",
      "\n",
      "Epoch 1222\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587297 \n",
      "\n",
      "Epoch 1223\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587380 \n",
      "\n",
      "Epoch 1224\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587358 \n",
      "\n",
      "Epoch 1225\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587381 \n",
      "\n",
      "Epoch 1226\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587327 \n",
      "\n",
      "Epoch 1227\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587251 \n",
      "\n",
      "Epoch 1228\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587288 \n",
      "\n",
      "Epoch 1229\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587282 \n",
      "\n",
      "Epoch 1230\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587257 \n",
      "\n",
      "Epoch 1231\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587323 \n",
      "\n",
      "Epoch 1232\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587256 \n",
      "\n",
      "Epoch 1233\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587241 \n",
      "\n",
      "Epoch 1234\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587257 \n",
      "\n",
      "Epoch 1235\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587245 \n",
      "\n",
      "Epoch 1236\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587301 \n",
      "\n",
      "Epoch 1237\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587228 \n",
      "\n",
      "Epoch 1238\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587217 \n",
      "\n",
      "Epoch 1239\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587235 \n",
      "\n",
      "Epoch 1240\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587229 \n",
      "\n",
      "Epoch 1241\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587220 \n",
      "\n",
      "Epoch 1242\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587245 \n",
      "\n",
      "Epoch 1243\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587192 \n",
      "\n",
      "Epoch 1244\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587199 \n",
      "\n",
      "Epoch 1245\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587199 \n",
      "\n",
      "Epoch 1246\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587220 \n",
      "\n",
      "Epoch 1247\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587196 \n",
      "\n",
      "Epoch 1248\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587211 \n",
      "\n",
      "Epoch 1249\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587196 \n",
      "\n",
      "Epoch 1250\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587120 \n",
      "\n",
      "Epoch 1251\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Avg loss: 0.587184 \n",
      "\n",
      "Epoch 1252\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dbc0093626b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-dbc0093626b6>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LRCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LRCN, self).__init__()\n",
    "        self.layer_1 = nn.Conv2d(3,15,5,stride=2)\n",
    "        self.layer_2 = nn.Conv2d(15,13,5,stride=2)\n",
    "        self.layer_3 = nn.Conv2d(13,7,5,stride=2)\n",
    "        self.layer_4 = nn.Conv2d(7,3,5,stride=2)\n",
    "        self.layer_5 = nn.Conv2d(3,1,5,stride=2)\n",
    "        self.layer_6 = nn.LSTM(252,100,2, batch_first = True)\n",
    "        self.layer_7 = nn.Linear(100, 9)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.length = x_train.shape[1]\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((-1, 3, 481, 793))\n",
    "        x = self.layer_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_5(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.reshape((self.batch_size, self.length, x.shape[-3],x.shape[-2],x.shape[-1]))\n",
    "        x = x.reshape((self.batch_size, self.length, x.shape[-3]*x.shape[-2]*x.shape[-1]))\n",
    "        x, (h_n, c_n) = self.layer_6(x)\n",
    "        x = self.relu(h_n[-1,:,:].reshape((self.batch_size,-1)))\n",
    "        x = self.layer_7(x)\n",
    "        return x\n",
    "\n",
    "model = LRCN().cuda()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss_result = loss(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss_result.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "def test(dataloader, model, loss):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader, model, loss, optimizer)\n",
    "    test(dataloader_test, model, loss)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ModelOutputs_resnet():\n",
    "    def __init__(self, model, target_layers, target_sub_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.target_sub_layers = target_sub_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        for name, module in self.model.named_children(): # 모든 layer에 대해서 직접 접근\n",
    "            x = module(x)\n",
    "            if name== 'avgpool': # avgpool이후 fully connect하기 전 data shape을 flatten시킴\n",
    "                x = torch.flatten(x,1)\n",
    "            if name in self.target_layers: # target_layer라면 해당 layer에서의 gradient를 저장\n",
    "                for sub_name, sub_module in module[len(module)-1].named_children():\n",
    "                    if sub_name in self.target_sub_layers:\n",
    "                        x.register_hook(self.save_gradient) #\n",
    "                        target_feature_maps = x # x's shape = 512X14X14(C,W,H) feature map\n",
    "        return target_feature_maps, x # target_activation : target_activation_layer's feature maps // output : classification ( ImageNet's classes : 1000 )\n",
    "\n",
    "\n",
    "class GradCam_resnet:\n",
    "    def __init__(self, model, target_layer_names,target_sub_layer_names, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:  # GPU일 경우 model을 cuda로 설정\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs_resnet(self.model, target_layer_names,target_sub_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index=None):\n",
    "\n",
    "        if self.cuda:  # GPU일 경우 input을 cuda로 변환하여 전달\n",
    "            features, output = self.extractor(input.cuda())\n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "\n",
    "        probs,idx = 0, 0\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())  # index = 정답이라고 추측한 class index\n",
    "            h_x = F.softmax(output,dim=1).data.squeeze()\n",
    "            probs, idx = h_x.sort(0,True)\n",
    "\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1  # 정답이라고 생각하는 class의 index 리스트 위치의 값만 1로\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)  # numpy배열을 tensor로 변환\n",
    "        # requires_grad == True 텐서의 모든 연산에 대하여 추적\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "\n",
    "        target = features  # A^k\n",
    "\n",
    "        target_cam = target.cpu().data.numpy()\n",
    "        bz, nc, h,w = target_cam.shape\n",
    "\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        params = list(self.model.parameters())\n",
    "\n",
    "        weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "        cam = weight_softmax[index].dot(target_cam.reshape((nc,h*w)))\n",
    "        cam = cam.reshape(h,w)\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (224, 224))  # 224X224크기로 변환\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]  # 논문에서의 global average pooling 식에 해당하는 부분\n",
    "        grad_cam = np.zeros(target.shape[1:], dtype=np.float32)  # 14X14\n",
    "\n",
    "        for i, w in enumerate(weights): # calcul grad_cam\n",
    "            grad_cam += w * target[i, :, :]  # linear combination L^c_{Grad-CAM}에 해당하는 식에서 ReLU를 제외한 식\n",
    "\n",
    "        grad_cam = np.maximum(grad_cam, 0)  # 0보다 작은 값을 제거\n",
    "        grad_cam = cv2.resize(grad_cam, (224, 224))  # 224X224크기로 변환\n",
    "        grad_cam = grad_cam - np.min(grad_cam)  #\n",
    "        grad_cam = grad_cam / np.max(grad_cam)  # 위의 것과 해당 줄의 것은 0~1사이의 값으로 정규화하기 위한 정리\n",
    "        return grad_cam, cam ,index,probs,idx ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FeatureExtractor_vgg():\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers): # target_layers = 35 ==> VGG19에서 가장 마지막 MaxPool2D전 ReLU함수\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items(): # 모든 layer에 대해서 직접 접근\n",
    "            x = module(x)\n",
    "            if name in self.target_layers: # target_layer라면 해당 layer에서의 gradient를 저장\n",
    "                x.register_hook(self.save_gradient) #\n",
    "                target_feature_maps = x # x's shape = 512X14X14(C,W,H) feature map\n",
    "        return target_feature_maps, x\n",
    "\n",
    "\n",
    "class ModelOutputs_vgg():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor_vgg(self.model.features, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations, output = self.feature_extractor(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.model.classifier(output) # feature extract를 통해서 나온 값을 활용하여 classification 진행\n",
    "        #print(\"ModelOutputs().output.shape : \",output[0])\n",
    "        #print(\"ModelOutputs().target_activations.shape :\",target_activations[0])\n",
    "        return target_activations, output\n",
    "\n",
    "class GradCam_vgg:\n",
    "    def __init__(self, model, target_layer_names, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda: # GPU일 경우 model을 cuda로 설정\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs_vgg(self.model, target_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index=None):\n",
    "        if self.cuda: # GPU일 경우 input을 cuda로 변환하여 전달\n",
    "            features, output = self.extractor(input.cuda())\n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "        #print(\"features : \",features.cpu().data.numpy().shape) # 해당 위치에서 추출된 feature map ( 512,14,14 ) (ChannelX14X14)\n",
    "        #print(\"output : \",output.cpu().data.numpy().shape) # class를 의미함\n",
    "        probs, idx = 0,0\n",
    "        #print(\"index : \", index)\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())  # index = 정답이라고 추측한 class index\n",
    "            h_x = F.softmax(output,dim=1).data.squeeze()\n",
    "            probs, idx = h_x.sort(0,True)\n",
    "        #print(\"index : \", index)\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1 # 정답이라고 생각하는 class의 index 리스트 위치의 값만 1로\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True) # numpy배열을 tensor로 변환\n",
    "        # requires_grad == True 텐서의 모든 연산에 대하여 추적\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "        #print(\"grads_val : \",grads_val.shape) # 512 X 14 X 14\n",
    "        target = features  # A^k\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        cam = None\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]  # 논문에서의 global average pooling 식에 해당하는 부분\n",
    "        grad_cam = np.zeros(target.shape[1:], dtype=np.float32)  # 14X14\n",
    "\n",
    "        for i, w in enumerate(weights):  # calcul grad_cam\n",
    "            grad_cam += w * target[i, :, :]  # linear combination L^c_{Grad-CAM}에 해당하는 식에서 ReLU를 제외한 식\n",
    "\n",
    "        grad_cam = np.maximum(grad_cam, 0)  # 0보다 작은 값을 제거\n",
    "        grad_cam = cv2.resize(grad_cam, (224, 224))  # 224X224크기로 변환\n",
    "        grad_cam = grad_cam - np.min(grad_cam)  #\n",
    "        grad_cam = grad_cam / np.max(grad_cam)  # 위의 것과 해당 줄의 것은 0~1사이의 값으로 정규화하기 위한 정리\n",
    "        return grad_cam, cam, index, probs, idx,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
