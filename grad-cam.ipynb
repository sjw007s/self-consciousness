{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_ready\n"
     ]
    }
   ],
   "source": [
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "ys_0=list()\n",
    "data_number= 20\n",
    "batch_size = 4\n",
    "epochs = 2000\n",
    "#100장정도.. 램 가능\n",
    "with open ('data/Data_narrow.txt', 'rt' ) as r_n:\n",
    "    target_r = r_n.readline()\n",
    "    for j in range(data_number): \n",
    "        #print(data_number)\n",
    "        xs_0=list()\n",
    "        for i in range(20): \n",
    "            temp_red=list()\n",
    "            temp_blue=list()\n",
    "            temp_green=list()\n",
    "            \n",
    "            r=open ('data/'+str(i+20*j)+'_building_r.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_red.append(target)\n",
    "                \n",
    "            r=open ('data/'+str(i+20*j)+'_building_g.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_green.append(target)\n",
    "                \n",
    "            r=open ('data/'+str(i+20*j)+'_building_b.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_blue.append(target)\n",
    "                \n",
    "            temp_red=np.array(temp_red, dtype=np.float32)\n",
    "            temp_greed=np.array(temp_green, dtype=np.float32)\n",
    "            temp_blue=np.array(temp_blue, dtype=np.float32)\n",
    "\n",
    "            xs_1=np.stack((temp_red, temp_green, temp_blue), axis=0)\n",
    "            xs_1=list(xs_1) \n",
    "            xs_0.append(xs_1)\n",
    "            \n",
    "            target_r = r_n.readline()\n",
    "            if i<20:\n",
    "                A_r,B_r,C_r,D_r,E_r,F_r,G_r,H_r,I_r,J_r,K_r=target_r.split(',')  \n",
    "\n",
    "                if i==0:\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    if i==1:\n",
    "                        ys_0.append([float(E_r)])\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        ys_0=ys_0[:-1]+[ys_0[-1]+[float(E_r)]]\n",
    "\n",
    "        xs_0_.append(xs_0)\n",
    "\n",
    "x_train = np.array(xs_0_, dtype=np.float32)\n",
    "y_train = np.array(ys_0, dtype=np.float32)\n",
    "x_train = torch.from_numpy(x_train).cuda()\n",
    "y_train = torch.from_numpy(y_train).cuda()\n",
    "\n",
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "\"\"\"\n",
    "for j in range(data_number): \n",
    "    xs_0=list()\n",
    "    for i in range(10): \n",
    "        temp_red=list()\n",
    "        temp_blue=list()\n",
    "        temp_green=list()\n",
    "\n",
    "        r=open ('data/'+str(i+10*j)+'_r.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_red.append(target)\n",
    "\n",
    "        r=open ('data/'+str(i+10*j)+'_g.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_green.append(target)\n",
    "\n",
    "        r=open ('data/'+str(i+10*j)+'_b.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_blue.append(target)\n",
    "\n",
    "        temp_red=np.array(temp_red, dtype=np.float32)\n",
    "        temp_greed=np.array(temp_green, dtype=np.float32)\n",
    "        temp_blue=np.array(temp_blue, dtype=np.float32)\n",
    "\n",
    "        xs_1=np.stack((temp_red, temp_green, temp_blue), axis=0)\n",
    "        xs_1=list(xs_1) \n",
    "        xs_0.append(xs_1)\n",
    "    \n",
    "    xs_0_.append(xs_0)\n",
    "        \n",
    "x_test = np.array(xs_0_, dtype=np.float32)\n",
    "x_test = torch.from_numpy(x_test).cuda()\n",
    "\"\"\"\n",
    "#print(x_train.shape,y_train.shape)\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#dataset_test = TensorDataset(x_test, y_train)\n",
    "#dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "#print(x_test.shape)\n",
    "\n",
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "ys_0=list()\n",
    "temp_red=list()\n",
    "temp_blue=list()\n",
    "temp_green=list()\n",
    "\n",
    "print(\"data_ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "0.0 2.7345846e-21\n",
      "[[3.3978398e-22 0.0000000e+00 7.3616156e-22 2.4034417e-21 0.0000000e+00\n",
      "  1.6639232e-21 1.2227005e-21]\n",
      " [1.7520386e-21 1.4529242e-21 2.7345846e-21 1.9566434e-21 1.6950843e-21\n",
      "  1.0433834e-22 4.9389099e-22]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-138906475e81>:121: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  result_ = (result_)/np.sum(result_).astype(np.int64)\n",
      "<ipython-input-28-138906475e81>:121: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result_ = (result_)/np.sum(result_).astype(np.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "0.0 8.45713e-40\n",
      "[[0.00000e+00 1.05581e-40 2.43115e-40 8.45713e-40 6.50322e-40 7.34108e-40\n",
      "  2.62235e-40]\n",
      " [1.79649e-40 3.62283e-40 0.00000e+00 0.00000e+00 4.73259e-40 3.65746e-40\n",
      "  1.10167e-40]]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "0.0 1.3673293e-24\n",
      "[[0.0000000e+00 1.7069200e-25 3.9306508e-25 1.3673293e-24 1.0514259e-24\n",
      "  1.1868875e-24 4.2397369e-25]\n",
      " [2.9045878e-25 5.8573534e-25 0.0000000e+00 0.0000000e+00 7.6515292e-25\n",
      "  5.9133245e-25 1.7811823e-25]]\n"
     ]
    }
   ],
   "source": [
    "class LRCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LRCN, self).__init__()\n",
    "        self.layer_1 = nn.Conv2d(3,5,10,stride=5)\n",
    "        self.layer_2 = nn.Conv2d(5,13,7,stride=3)\n",
    "        self.layer_3 = nn.Conv2d(13,20,7,stride=3)\n",
    "        #self.layer_4 = nn.Conv2d(20,30,5,stride=2)\n",
    "        #self.layer_5 = nn.Conv2d(30,40,5,stride=2)\n",
    "        self.layer_6 = nn.LSTM(280,200,1, batch_first = True)\n",
    "        self.layer_7 = nn.Linear(200, 1)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.length = x_train.shape[1]\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.a=0\n",
    "        self.b=0\n",
    "        self.c=0\n",
    "        #self.d=0\n",
    "        #self.e=0\n",
    "        self.count_=0\n",
    "        #self.o=0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((-1, 3, 200, 400))\n",
    "        x = self.layer_1(x)\n",
    "        self.a = self.relu(x)\n",
    "        x = self.layer_2(self.a)\n",
    "        self.b = self.relu(x)\n",
    "        x = self.layer_3(self.b)\n",
    "        self.c = self.relu(x)\n",
    "        #x = self.layer_4(self.c)\n",
    "        #self.d = self.relu(x)\n",
    "        #x = self.layer_5(self.d)\n",
    "        #self.e = self.relu(x)\n",
    "        x = self.c.reshape((self.batch_size, self.length, x.shape[-3],x.shape[-2],x.shape[-1]))\n",
    "        x = x.reshape((self.batch_size, self.length, x.shape[-3]*x.shape[-2]*x.shape[-1]))\n",
    "        x, (h_n, c_n) = self.layer_6(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.relu(h_n[-1,:,:].reshape((self.batch_size,-1)))\n",
    "        x = self.layer_7(x[:,1:,:]).reshape((self.batch_size,-1))\n",
    "        #print(x.shape)\n",
    "        #x.retain_grad()\n",
    "        self.count_+=1\n",
    "        #print(\"종우\",x.grad)\n",
    "        return x\n",
    "\n",
    "model = LRCN().cuda()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "#print(\"서\",model.parameters().shape)\n",
    "      \n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        \n",
    "        #print(pred.shape,y.shape)\n",
    "        loss_result = loss(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.a.retain_grad()\n",
    "        model.b.retain_grad()\n",
    "        model.c.retain_grad()\n",
    "        #model.d.retain_grad()\n",
    "        #model.e.retain_grad()\n",
    "        #model.o.retain_grad()\n",
    "        #print(model.e.grad)\n",
    "        loss_result.backward()\n",
    "        #print(model.e.requires_grad)\n",
    "        #print(model.e.grad)\n",
    "        #print(model.b.grad)\n",
    "        #print(model.c.grad)\n",
    "        #print(model.d.grad)\n",
    "        #print(model.e.grad)\n",
    "        #print(loss_result.sum())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"종우\",loss_result.register_hook())\n",
    "    temp_sum=0\n",
    "    #print(model.a.grad.shape)\n",
    "    width_ = model.c.grad.shape[-1]\n",
    "    #print(width_)\n",
    "    result_temp = torch.sum(model.c.grad,-1)\n",
    "    #print(result_temp)\n",
    "    height_ = result_temp.shape[-1]\n",
    "    #print(height_)\n",
    "    result_temp = torch.sum(result_temp ,-1)\n",
    "    #print(result_temp.shape, result_temp)\n",
    "    result_temp/=(width_*height_)\n",
    "    #print()\n",
    "    #print(result_temp.shape, result_temp)\n",
    "    #print(result_temp)\n",
    "    #print(param.shape)\n",
    "    #print(result_temp.shape)\n",
    "    #print(model.e.grad.shape, result_temp.shape)\n",
    "    if model.count_==100:\n",
    "        model.count_=0\n",
    "        for k in range(1):  \n",
    "        #for j in range(1):\n",
    "            #print(result_temp.shape[1])\n",
    "            #print(result_temp.shape[1])\n",
    "            #j=10\n",
    "            #for k in range(1): \n",
    "            temp_sum=0\n",
    "            for j in range(result_temp.shape[1]):\n",
    "                k=10\n",
    "                #print(result_temp[k,j].shape, model.a.grad.shape)\n",
    "                #print(model.a.grad[j].shape)\n",
    "                #print(i,j,k)\n",
    "                #print(result_temp[k,j].shape,model.e.grad[k,j,:,:].shape)\n",
    "                temp_sum += result_temp[k,j]*model.c.grad[k,j,:,:]\n",
    "            result_ = model.relu(temp_sum)\n",
    "            result_ = result_.cpu().numpy()\n",
    "            min_= np.min(result_)\n",
    "            max_ = np.max(result_)\n",
    "            print(min_,max_)\n",
    "            print(result_)\n",
    "            result_ = (result_)/np.sum(result_).astype(np.int64)\n",
    "            pil_image=Image.fromarray(result_)\n",
    "            pil_image.show()\n",
    "            with open(\"data/\"+str(i)+\"_r.csv\",'a',newline='') as aa:\n",
    "                writer_a = csv.writer(aa, delimiter=',')\n",
    "\n",
    "            #print(temp_sum)\n",
    "    #print(temp_sum.shape, temp_sum)\n",
    "    #result_ = model.relu(temp_sum)\n",
    "    #print(result_.shape)\n",
    "    #temp_sum= self.relu(temp_sum)\n",
    "    #result_ = result_.cpu().numpy()\n",
    "    #min_= np.min(result_)\n",
    "    #max_ = np.max(result_)\n",
    "    #print(min_,max_)\n",
    "    #pil_image=Image.fromarray(result_)\n",
    "    #pil_image.show()\n",
    "    #print(result_)\n",
    "    #print(model.e.grad)\n",
    "    \n",
    "        \n",
    "    optimizer.step()\n",
    "            \n",
    "def test(dataloader, model, loss):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader, model, loss, optimizer)\n",
    "    #test(dataloader_test, model, loss)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class ModelOutputs_resnet():\n",
    "    def __init__(self, model, target_layers, target_sub_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.target_sub_layers = target_sub_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        for name, module in self.model.named_children(): # 모든 layer에 대해서 직접 접근\n",
    "            x = module(x)\n",
    "            if name== 'avgpool': # avgpool이후 fully connect하기 전 data shape을 flatten시킴\n",
    "                x = torch.flatten(x,1)\n",
    "            if name in self.target_layers: # target_layer라면 해당 layer에서의 gradient를 저장\n",
    "                for sub_name, sub_module in module[len(module)-1].named_children():\n",
    "                    if sub_name in self.target_sub_layers:\n",
    "                        x.register_hook(self.save_gradient) #\n",
    "                        target_feature_maps = x # x's shape = 512X14X14(C,W,H) feature map\n",
    "        return target_feature_maps, x # target_activation : target_activation_layer's feature maps // output : classification ( ImageNet's classes : 1000 )\n",
    "\n",
    "\n",
    "class GradCam_resnet:\n",
    "    def __init__(self, model, target_layer_names,target_sub_layer_names, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda:  # GPU일 경우 model을 cuda로 설정\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs_resnet(self.model, target_layer_names,target_sub_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index=None):\n",
    "\n",
    "        if self.cuda:  # GPU일 경우 input을 cuda로 변환하여 전달\n",
    "            features, output = self.extractor(input.cuda())\n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "\n",
    "        probs,idx = 0, 0\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())  # index = 정답이라고 추측한 class index\n",
    "            h_x = F.softmax(output,dim=1).data.squeeze()\n",
    "            probs, idx = h_x.sort(0,True)\n",
    "\n",
    "\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1  # 정답이라고 생각하는 class의 index 리스트 위치의 값만 1로\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True)  # numpy배열을 tensor로 변환\n",
    "        # requires_grad == True 텐서의 모든 연산에 대하여 추적\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "\n",
    "        target = features  # A^k\n",
    "\n",
    "        target_cam = target.cpu().data.numpy()\n",
    "        bz, nc, h,w = target_cam.shape\n",
    "\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        params = list(self.model.parameters())\n",
    "\n",
    "        weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "        cam = weight_softmax[index].dot(target_cam.reshape((nc,h*w)))\n",
    "        cam = cam.reshape(h,w)\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cv2.resize(cam, (224, 224))  # 224X224크기로 변환\n",
    "        cam = cam - np.min(cam)\n",
    "        cam = cam / np.max(cam)\n",
    "\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]  # 논문에서의 global average pooling 식에 해당하는 부분\n",
    "        grad_cam = np.zeros(target.shape[1:], dtype=np.float32)  # 14X14\n",
    "\n",
    "        for i, w in enumerate(weights): # calcul grad_cam\n",
    "            grad_cam += w * target[i, :, :]  # linear combination L^c_{Grad-CAM}에 해당하는 식에서 ReLU를 제외한 식\n",
    "\n",
    "        grad_cam = np.maximum(grad_cam, 0)  # 0보다 작은 값을 제거\n",
    "        grad_cam = cv2.resize(grad_cam, (224, 224))  # 224X224크기로 변환\n",
    "        grad_cam = grad_cam - np.min(grad_cam)  #\n",
    "        grad_cam = grad_cam / np.max(grad_cam)  # 위의 것과 해당 줄의 것은 0~1사이의 값으로 정규화하기 위한 정리\n",
    "        return grad_cam, cam ,index,probs,idx ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class FeatureExtractor_vgg():\n",
    "    \"\"\" Class for extracting activations and\n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers): # target_layers = 35 ==> VGG19에서 가장 마지막 MaxPool2D전 ReLU함수\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients.append(grad)\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items(): # 모든 layer에 대해서 직접 접근\n",
    "            x = module(x)\n",
    "            if name in self.target_layers: # target_layer라면 해당 layer에서의 gradient를 저장\n",
    "                x.register_hook(self.save_gradient) #\n",
    "                target_feature_maps = x # x's shape = 512X14X14(C,W,H) feature map\n",
    "        return target_feature_maps, x\n",
    "\n",
    "\n",
    "class ModelOutputs_vgg():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor_vgg(self.model.features, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x):\n",
    "        target_activations, output = self.feature_extractor(x)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.model.classifier(output) # feature extract를 통해서 나온 값을 활용하여 classification 진행\n",
    "        #print(\"ModelOutputs().output.shape : \",output[0])\n",
    "        #print(\"ModelOutputs().target_activations.shape :\",target_activations[0])\n",
    "        return target_activations, output\n",
    "\n",
    "class GradCam_vgg:\n",
    "    def __init__(self, model, target_layer_names, use_cuda):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.cuda = use_cuda\n",
    "        if self.cuda: # GPU일 경우 model을 cuda로 설정\n",
    "            self.model = model.cuda()\n",
    "\n",
    "        self.extractor = ModelOutputs_vgg(self.model, target_layer_names)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "    def __call__(self, input, index=None):\n",
    "        if self.cuda: # GPU일 경우 input을 cuda로 변환하여 전달\n",
    "            features, output = self.extractor(input.cuda())\n",
    "        else:\n",
    "            features, output = self.extractor(input)\n",
    "        #print(\"features : \",features.cpu().data.numpy().shape) # 해당 위치에서 추출된 feature map ( 512,14,14 ) (ChannelX14X14)\n",
    "        #print(\"output : \",output.cpu().data.numpy().shape) # class를 의미함\n",
    "        probs, idx = 0,0\n",
    "        #print(\"index : \", index)\n",
    "        if index == None:\n",
    "            index = np.argmax(output.cpu().data.numpy())  # index = 정답이라고 추측한 class index\n",
    "            h_x = F.softmax(output,dim=1).data.squeeze()\n",
    "            probs, idx = h_x.sort(0,True)\n",
    "        #print(\"index : \", index)\n",
    "        one_hot = np.zeros((1, output.size()[-1]), dtype=np.float32)\n",
    "        one_hot[0][index] = 1 # 정답이라고 생각하는 class의 index 리스트 위치의 값만 1로\n",
    "        one_hot = torch.from_numpy(one_hot).requires_grad_(True) # numpy배열을 tensor로 변환\n",
    "        # requires_grad == True 텐서의 모든 연산에 대하여 추적\n",
    "        if self.cuda:\n",
    "            one_hot = torch.sum(one_hot.cuda() * output)\n",
    "        else:\n",
    "            one_hot = torch.sum(one_hot * output)\n",
    "\n",
    "        self.model.features.zero_grad()\n",
    "        self.model.classifier.zero_grad()\n",
    "        one_hot.backward(retain_graph=True)\n",
    "\n",
    "        grads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "        #print(\"grads_val : \",grads_val.shape) # 512 X 14 X 14\n",
    "        target = features  # A^k\n",
    "        target = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "        cam = None\n",
    "\n",
    "        weights = np.mean(grads_val, axis=(2, 3))[0, :]  # 논문에서의 global average pooling 식에 해당하는 부분\n",
    "        grad_cam = np.zeros(target.shape[1:], dtype=np.float32)  # 14X14\n",
    "\n",
    "        for i, w in enumerate(weights):  # calcul grad_cam\n",
    "            grad_cam += w * target[i, :, :]  # linear combination L^c_{Grad-CAM}에 해당하는 식에서 ReLU를 제외한 식\n",
    "\n",
    "        grad_cam = np.maximum(grad_cam, 0)  # 0보다 작은 값을 제거\n",
    "        grad_cam = cv2.resize(grad_cam, (224, 224))  # 224X224크기로 변환\n",
    "        grad_cam = grad_cam - np.min(grad_cam)  #\n",
    "        grad_cam = grad_cam / np.max(grad_cam)  # 위의 것과 해당 줄의 것은 0~1사이의 값으로 정규화하기 위한 정리\n",
    "        return grad_cam, cam, index, probs, idx,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
