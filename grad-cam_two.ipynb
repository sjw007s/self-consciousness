{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Device: cuda\n",
      "Current cuda device: 0\n",
      "Count of using GPUs: 2\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "from PIL import Image\n",
    "import time\n",
    "import cv2 as cv\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "torch.Size([20, 20, 3, 100, 200]) torch.Size([20, 19])\n",
      "data_ready\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "ys_0=list()\n",
    "data_number= 20\n",
    "batch_size = 4\n",
    "epochs = 20000\n",
    "#100장정도.. 램 가능\n",
    "with open ('data/Data_narrow.txt', 'rt' ) as r_n:\n",
    "    target_r = r_n.readline()\n",
    "    for j in range(data_number): \n",
    "        print(j)\n",
    "        xs_0=list()\n",
    "        for i in range(20): \n",
    "            temp_red=list()\n",
    "            temp_blue=list()\n",
    "            temp_green=list()\n",
    "            \n",
    "            r=open ('data/'+str(i+20*j)+'_building_r.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_red.append(target)\n",
    "                \n",
    "            r=open ('data/'+str(i+20*j)+'_building_g.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_green.append(target)\n",
    "                \n",
    "            r=open ('data/'+str(i+20*j)+'_building_b.csv', 'r' )\n",
    "            rdr=csv.reader(r)\n",
    "            for target in rdr:\n",
    "                temp_blue.append(target)\n",
    "                \n",
    "            temp_red=np.array(temp_red, dtype=np.float32)\n",
    "            temp_greed=np.array(temp_green, dtype=np.float32)\n",
    "            temp_blue=np.array(temp_blue, dtype=np.float32)\n",
    "\n",
    "            xs_1=np.stack((temp_red, temp_green, temp_blue), axis=0)\n",
    "            xs_1=list(xs_1) \n",
    "            xs_0.append(xs_1)\n",
    "            \n",
    "            target_r = r_n.readline()\n",
    "            if i<20:\n",
    "                A_r,B_r,C_r,D_r,E_r,F_r,G_r,H_r,I_r,J_r,K_r=target_r.split(',')  \n",
    "\n",
    "                if i==0:\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    if i==1:\n",
    "                        ys_0.append([float(E_r)])\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        ys_0=ys_0[:-1]+[ys_0[-1]+[float(E_r)]]\n",
    "\n",
    "        xs_0_.append(xs_0)\n",
    "\n",
    "x_train = np.array(xs_0_, dtype=np.float32)\n",
    "y_train = np.array(ys_0, dtype=np.float32)\n",
    "x_train = torch.from_numpy(x_train).cuda()\n",
    "y_train = torch.from_numpy(y_train).cuda()\n",
    "\n",
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "\n",
    "for j in range(data_number): \n",
    "    xs_0=list()\n",
    "    for i in range(20): \n",
    "        temp_red=list()\n",
    "        temp_blue=list()\n",
    "        temp_green=list()\n",
    "\n",
    "        r=open ('data/'+str(i+20*j)+'_r.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_red.append(target)\n",
    "\n",
    "        r=open ('data/'+str(i+20*j)+'_g.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_green.append(target)\n",
    "\n",
    "        r=open ('data/'+str(i+20*j)+'_b.csv', 'r' )\n",
    "        rdr=csv.reader(r)\n",
    "        for target in rdr:\n",
    "            temp_blue.append(target)\n",
    "\n",
    "        temp_red=np.array(temp_red, dtype=np.float32)\n",
    "        temp_greed=np.array(temp_green, dtype=np.float32)\n",
    "        temp_blue=np.array(temp_blue, dtype=np.float32)\n",
    "\n",
    "        xs_1=np.stack((temp_red, temp_green, temp_blue), axis=0)\n",
    "        xs_1=list(xs_1) \n",
    "        xs_0.append(xs_1)\n",
    "    \n",
    "    xs_0_.append(xs_0)\n",
    "        \n",
    "x_test = np.array(xs_0_, dtype=np.float32)\n",
    "#print(x_test.shape)\n",
    "#for i in range(100):\n",
    "#    print(x_test[0,0,0,i,:])\n",
    "x_test = torch.from_numpy(x_test).cuda()\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_train)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "#print(x_test.shape)\n",
    "\n",
    "xs_0=list()\n",
    "xs_0_=list()\n",
    "xs_1=list()\n",
    "ys_0=list()\n",
    "temp_red=list()\n",
    "temp_blue=list()\n",
    "temp_green=list()\n",
    "\n",
    "print(\"data_ready\")\n",
    "print(np.array_equal(x_train,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "tensor(466.0328, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(416.9989, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(465.4784, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(427.1852, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(407.5099, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4737e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4758e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4658e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4682e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4752e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "tensor(4.4374e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4258e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4248e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4118e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4106e+08, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0831e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4061e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(8.1270e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2114e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2608e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "tensor(6.4674e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(7.0222e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6450e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2234e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2602e+09, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.0160, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6443, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.5615, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.0867, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.3785, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "tensor(10.7416, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.8317, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4071, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0460, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6607, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6839, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.2429, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.5948, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.5614, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6033, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "tensor(13.2564, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.7666, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0530, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8439, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7664, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7988, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8424, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.5970, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6066, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8408, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "tensor(11.9665, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.2469, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.2101, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.4464, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8156, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7846, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4425, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.2843, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.5437, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6297, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "tensor(13.1975, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3549, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.1732, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.3379, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6213, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6974, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2446, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3536, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.9188, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4695, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "tensor(11.2480, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.8263, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.9622, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1779, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4695, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.9849, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.8995, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.2498, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.5332, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0158, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "tensor(9.5890, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.2134, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2101, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.9054, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7652, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.2775, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.2295, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8382, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.5757, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7614, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "tensor(10.0607, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.5489, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.5383, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.9688, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.5656, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.3525, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8556, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.5687, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.7615, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.1432, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "tensor(12.9212, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.4133, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.3162, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.8511, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.1797, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')\n",
      "train tensor([0., 0., 0., 0., 0.], device='cuda:1')\n",
      "tensor(12.9715, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7316, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8665, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.0855, device='cuda:1', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-934a2870cdd9>:289: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result_ = ((255*result_)/max_).astype(np.uint8)\n",
      "<ipython-input-7-934a2870cdd9>:314: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result_ = ((255*result_)/max_).astype(np.uint8)\n",
      "<ipython-input-7-934a2870cdd9>:374: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result_ = ((255*result_)/max_).astype(np.uint8)\n",
      "<ipython-input-7-934a2870cdd9>:399: RuntimeWarning: invalid value encountered in true_divide\n",
      "  result_ = ((255*result_)/max_).astype(np.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.0257, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "test tensor([0., 0., 0., 0., 0.], device='cuda:1')\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "tensor(12.8229, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4098, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.9593, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.4014, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.0872, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.1818, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7942, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6145, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.0910, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.9984, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "tensor(9.6850, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.7225, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6886, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6997, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8841, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4975, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4853, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.8235, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.1337, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.7391, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "tensor(13.7000, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8315, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.8441, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.1462, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.1573, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2080, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0225, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.1955, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.0900, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.1623, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "tensor(11.5429, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7606, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.5929, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.4939, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.2881, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8842, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3007, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.7311, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.2644, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4972, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "tensor(11.1874, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.1396, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6293, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.1237, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.5976, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0469, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8551, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(15.4024, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6754, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.6969, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "tensor(12.1066, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.3754, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.1103, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.5835, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.5008, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.0681, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6593, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.2212, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.2611, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.4662, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "tensor(12.7229, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.3026, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7298, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.9697, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.9509, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2330, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.5230, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.5129, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6236, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7826, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "tensor(14.4090, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.3891, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.9583, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.1843, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7344, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.7693, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0688, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.2969, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2602, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.2790, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "tensor(12.2144, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.1723, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.3721, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7342, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.1812, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8172, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.0970, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(8.9998, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.0486, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.7109, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "tensor(10.4360, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.0351, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0110, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7486, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4427, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6886, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.1680, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.6555, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.5548, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.6058, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "tensor(12.9983, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8351, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4942, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7286, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.6165, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')\n",
      "train tensor([0., 0., 0., 0., 0.], device='cuda:1')\n",
      "tensor(11.1812, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.9036, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7387, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.7564, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.0920, device='cuda:1', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test tensor([0., 0., 0., 0., 0.], device='cuda:1')\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "tensor(12.4630, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.4027, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6281, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2632, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.9150, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4541, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(8.8394, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6570, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.2962, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(14.4243, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "tensor(12.7993, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.7775, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.7726, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1427, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.1790, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.0501, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.9685, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.2647, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4827, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.9044, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "tensor(13.1058, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(13.4787, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7083, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.4969, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8806, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(12.7630, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.0165, device='cuda:1', grad_fn=<MseLossBackward0>)\n",
      "tensor(10.5833, device='cuda:1', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-934a2870cdd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-934a2870cdd9>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(dataloader_2, model, loss, optimizer)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mloss_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;31m#print(\"test\",pred,y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretain_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;31m# All strings are unicode in Python 3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m    388\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                     \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;31m# Check if scientific representation should be used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mnonzero_finite_max\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnonzero_finite_min\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1000.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                         \u001b[1;32mor\u001b[0m \u001b[0mnonzero_finite_max\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1.e8\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                         \u001b[1;32mor\u001b[0m \u001b[0mnonzero_finite_min\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1.e-4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LRCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LRCN, self).__init__()\n",
    "        self.layer_1 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_2 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_3 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_4 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_5 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_6 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_7 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_8 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_9 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_10 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_11 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_12 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_13 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_14 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_15 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_16 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_17 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_18 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_19 = nn.Conv2d(3,5,5,stride=1)\n",
    "        self.layer_20 = nn.Conv2d(3,5,5,stride=1)\n",
    "        \n",
    "        self.layer_21 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_22 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_23 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_24 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_25 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_26 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_27 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_28 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_29 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_30 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_31 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_32 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_33 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_34 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_35 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_36 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_37 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_38 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_39 = nn.Conv2d(5,2,5,stride=2)\n",
    "        self.layer_40 = nn.Conv2d(5,2,5,stride=2)\n",
    "        \n",
    "        self.layer_41 = nn.Linear(176640, 1)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.length = x_train.shape[1]\n",
    "        self.relu = nn.ReLU()\n",
    "        self.test_on=0\n",
    "        self.a=0 #1\n",
    "        self.b=0 #2\n",
    "        self.c=0 #3\n",
    "        self.d=0 #4\n",
    "        self.e=0 #5\n",
    "        self.f=0 #6\n",
    "        self.g=0 #7\n",
    "        self.h=0 #8\n",
    "        self.i=0 #9\n",
    "        self.j=0 #10\n",
    "        self.k=0 #11\n",
    "        self.l=0 #12\n",
    "        self.m=0 #13\n",
    "        self.n=0 #14\n",
    "        self.o=0 #15\n",
    "        self.p=0 #16\n",
    "        self.q=0 #17\n",
    "        self.r=0 #!8\n",
    "        self.s=0 #19\n",
    "        self.t=0 #20\n",
    "        self.u=0 #final\n",
    "        \n",
    "        self.a1=0 #1\n",
    "        self.b1=0 #2\n",
    "        self.c1=0 #3\n",
    "        self.d1=0 #4\n",
    "        self.e1=0 #5\n",
    "        self.f1=0 #6\n",
    "        self.g1=0 #7\n",
    "        self.h1=0 #8\n",
    "        self.i1=0 #9\n",
    "        self.j1=0 #10\n",
    "        self.k1=0 #11\n",
    "        self.l1=0 #12\n",
    "        self.m1=0 #13\n",
    "        self.n1=0 #14\n",
    "        self.o1=0 #15\n",
    "        self.p1=0 #16\n",
    "        self.q1=0 #17\n",
    "        self.r1=0 #!8\n",
    "        self.s1=0 #19\n",
    "        self.t1=0 #20\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.count_=0\n",
    "        self.temp_data=0\n",
    "        self.temp_result=0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x_1 = self.layer_1(x[:,0,:,:,:])\n",
    "        self.a = self.relu(x_1)\n",
    "        x_2 = self.layer_2(x[:,1,:,:,:])\n",
    "        self.b = self.relu(x_2)\n",
    "        x_3 = self.layer_3(x[:,2,:,:,:])\n",
    "        self.c = self.relu(x_3)\n",
    "        x_4 = self.layer_4(x[:,3,:,:,:])\n",
    "        self.d = self.relu(x_4)\n",
    "        x_5 = self.layer_5(x[:,4,:,:,:])\n",
    "        self.e = self.relu(x_5)\n",
    "        x_6 = self.layer_6(x[:,5,:,:,:])\n",
    "        self.f = self.relu(x_6)\n",
    "        x_7 = self.layer_7(x[:,6,:,:,:])\n",
    "        self.g = self.relu(x_7)\n",
    "        x_8 = self.layer_8(x[:,7,:,:,:])\n",
    "        self.h = self.relu(x_8)\n",
    "        x_9 = self.layer_9(x[:,8,:,:,:])\n",
    "        self.i = self.relu(x_9)\n",
    "        x_10 = self.layer_10(x[:,9,:,:,:])\n",
    "        self.j = self.relu(x_10)\n",
    "        x_11 = self.layer_11(x[:,10,:,:,:])\n",
    "        self.k = self.relu(x_11)\n",
    "        x_12 = self.layer_12(x[:,11,:,:,:])\n",
    "        self.l = self.relu(x_12)\n",
    "        x_13 = self.layer_13(x[:,12,:,:,:])\n",
    "        self.m = self.relu(x_13)\n",
    "        x_14 = self.layer_14(x[:,13,:,:,:])\n",
    "        self.n = self.relu(x_14)\n",
    "        x_15 = self.layer_15(x[:,14,:,:,:])\n",
    "        self.o = self.relu(x_15)\n",
    "        x_16 = self.layer_16(x[:,15,:,:,:])\n",
    "        self.p = self.relu(x_16)\n",
    "        x_17 = self.layer_17(x[:,16,:,:,:])\n",
    "        self.q = self.relu(x_17)\n",
    "        x_18 = self.layer_18(x[:,17,:,:,:])\n",
    "        self.r = self.relu(x_18)\n",
    "        x_19 = self.layer_19(x[:,18,:,:,:])\n",
    "        self.s = self.relu(x_19)\n",
    "        x_20 = self.layer_20(x[:,19,:,:,:])\n",
    "        self.t = self.relu(x_20)\n",
    "        \n",
    "        x_1_1 = self.layer_21(self.a)\n",
    "        result_1 = self.relu(x_1_1)\n",
    "        x_2_1 = self.layer_22(self.b)\n",
    "        result_2 = self.relu(x_2_1)\n",
    "        x_3_1 = self.layer_23(self.c)\n",
    "        result_3 = self.relu(x_3_1)\n",
    "        x_4_1 = self.layer_24(self.d)\n",
    "        result_4 = self.relu(x_4_1)\n",
    "        x_5_1 = self.layer_25(self.e)\n",
    "        result_5 = self.relu(x_5_1)\n",
    "        x_6_1 = self.layer_26(self.f)\n",
    "        result_6 = self.relu(x_6_1)\n",
    "        x_7_1 = self.layer_27(self.g)\n",
    "        result_7 = self.relu(x_7_1)\n",
    "        x_8_1 = self.layer_28(self.h)\n",
    "        result_8 = self.relu(x_8_1)\n",
    "        x_9_1 = self.layer_29(self.i)\n",
    "        result_9 = self.relu(x_9_1)\n",
    "        x_10_1 = self.layer_30(self.j)\n",
    "        result_10 = self.relu(x_10_1)\n",
    "        x_11_1 = self.layer_31(self.k)\n",
    "        result_11 = self.relu(x_11_1)\n",
    "        x_12_1 = self.layer_32(self.l)\n",
    "        result_12 = self.relu(x_12_1)\n",
    "        x_13_1 = self.layer_33(self.m)\n",
    "        result_13 = self.relu(x_13_1)\n",
    "        x_14_1 = self.layer_34(self.n)\n",
    "        result_14 = self.relu(x_14_1)\n",
    "        x_15_1 = self.layer_35(self.o)\n",
    "        result_15 = self.relu(x_15_1)\n",
    "        x_16_1 = self.layer_36(self.p)\n",
    "        result_16 = self.relu(x_16_1)\n",
    "        x_17_1 = self.layer_37(self.q)\n",
    "        result_17 = self.relu(x_17_1)\n",
    "        x_18_1 = self.layer_38(self.r)\n",
    "        result_18 = self.relu(x_18_1)\n",
    "        x_19_1 = self.layer_39(self.s)\n",
    "        result_19 = self.relu(x_19_1)\n",
    "        x_20_1 = self.layer_40(self.t)\n",
    "        result_20 = self.relu(x_20_1)\n",
    "        \n",
    "        \n",
    "        self.u = torch.cat((result_1.reshape((self.batch_size, -1)),result_2.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_3.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_4.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_5.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_6.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_7.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_8.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_9.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_10.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_11.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_12.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_13.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_14.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_15.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_16.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_17.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_18.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_19.reshape((self.batch_size, -1))),axis=1)\n",
    "        self.u = torch.cat((self.u.reshape((self.batch_size, -1)),result_20.reshape((self.batch_size, -1))),axis=1)\n",
    "        \n",
    "\n",
    "        x= self.layer_41(self.u)\n",
    "    \n",
    "        \n",
    "        #x = self.relu(h_n[-1,:,:].reshape((self.batch_size,-1)))\n",
    "        #x = self.layer_7(self.o[:,1:,:]).reshape((self.batch_size,-1))\n",
    "        #print(x.shape)\n",
    "        #x.retain_grad()\n",
    "        self.count_+=1\n",
    "        #print(self.count_)\n",
    "        #print(\"종우\",x.grad)\n",
    "        return x\n",
    "\n",
    "model = LRCN().cuda()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "#print(\"서\",model.parameters().shape)\n",
    "      \n",
    "def train(dataloader_1, model, loss, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for batch, (X, y) in enumerate(dataloader_1):\n",
    "        pred = model(X)\n",
    "        model.temp_data = X[0]\n",
    "        #print(pred,y)\n",
    "        loss_result = loss(pred, y)\n",
    "        #print(model.a.grad.shape[-1])\n",
    "        #print(\"training\",pred)\n",
    "        print(loss_result)\n",
    "        model.temp_result=loss_result\n",
    "        model.a.retain_grad()\n",
    "        model.b.retain_grad()\n",
    "        model.c.retain_grad()\n",
    "        \n",
    "        #model.d.retain_grad()\n",
    "        #model.e.retain_grad()\n",
    "        #model.o.retain_grad()\n",
    "        #print(model.e.grad)\n",
    "        loss_result.backward()\n",
    "        #print(model.e.requires_grad)\n",
    "        #print(model.e.grad)\n",
    "        #print(model.b.grad)\n",
    "        #print(model.c.grad)\n",
    "        #print(model.d.grad)\n",
    "        #print(model.e.grad)\n",
    "        #print(loss_result.sum())\n",
    "        #print(model.a.grad())\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"종우\",loss_result.register_hook())\n",
    "    \n",
    "    #print(result_temp.shape, result_temp)\n",
    "    #print(result_temp)\n",
    "    #print(param.shape)\n",
    "    #print(result_temp.shape)\n",
    "    #print(model.)\n",
    "    #print(model.count_)\n",
    "    #print(model.e.grad.shape, result_temp.shape)\n",
    "    if model.count_==105:\n",
    "        temp_sum=0\n",
    "        model.test_on=1\n",
    "        #print(model.a.grad.shape)\n",
    "        #width_ = model.a.grad.shape[-1]\n",
    "        result_temp = torch.sum(model.a.grad.detach(),-1)\n",
    "        print(model.a.grad.detach())\n",
    "        height_ = result_temp.shape[-1]\n",
    "        result_temp = torch.sum(result_temp,-1)\n",
    "        result_temp_ = torch.sum(result_temp,0)\n",
    "        result_temp = result_temp_/5\n",
    "        model.count_=0\n",
    "        temp_sum=0\n",
    "        print(\"train\",result_temp)\n",
    "        #print(result_temp.shape)\n",
    "        for k in range(1):  \n",
    "            for j in range(result_temp.shape[0]):\n",
    "                k=0\n",
    "                temp_grad = model.a.grad.detach().reshape((model.batch_size, 5, model.a.grad.shape[-2],model.a.grad.shape[-1]))\n",
    "                temp_sum += result_temp[j]*(temp_grad[0,j,:,:])\n",
    "            result_ = temp_sum.detach().cpu().numpy()\n",
    "            min_= np.min(result_)\n",
    "            result_=result_-min_\n",
    "            max_ = np.max(result_)\n",
    "            result_ = ((255*result_)/max_).astype(np.uint8)\n",
    "            \n",
    "            pil_image=Image.fromarray(result_)\n",
    "            #pil_name=str(k)+str(j)+\".png\"\n",
    "            result_=pil_image.resize((200,100))\n",
    "            pil_image.save(\"name.png\")\n",
    "    \n",
    "        temp_sum=0\n",
    "        width_ = model.b.grad.shape[-1]\n",
    "        result_temp = torch.sum(model.b.grad.detach(),-1)\n",
    "        height_ = result_temp.shape[-1]\n",
    "        result_temp = torch.sum(result_temp,-1)\n",
    "        result_temp_ = torch.sum(result_temp,0)\n",
    "        result_temp = result_temp_/5\n",
    "        model.count_=0\n",
    "        temp_sum=0\n",
    "        for k in range(1):  \n",
    "            for j in range(result_temp.shape[0]):\n",
    "                k=1\n",
    "                temp_grad = model.b.grad.detach().reshape((model.batch_size, 5, model.b.grad.shape[-2],model.b.grad.shape[-1]))\n",
    "                temp_sum += result_temp[j]*(temp_grad[0,j,:,:])\n",
    "            result_ = temp_sum.detach().cpu().numpy()\n",
    "            min_= np.min(result_)\n",
    "            result_=result_-min_\n",
    "            max_ = np.max(result_)\n",
    "            result_ = ((255*result_)/max_).astype(np.uint8)\n",
    "            #for t in range(96):\n",
    "                \n",
    "            #    print(result_[t,:])\n",
    "            pil_image=Image.fromarray(result_)\n",
    "            #pil_name=str(k)+str(j)+\".png\"\n",
    "            result_=pil_image.resize((200,100))\n",
    "            pil_image.save(\"name_middle.png\")\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #time.sleep(0.001)\n",
    "        \n",
    "    optimizer.step()\n",
    "            \n",
    "def test(dataloader_2, model, loss, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    size = len(dataloader_2.dataset)\n",
    "    num_batches = len(dataloader_2)\n",
    "    #model.eval()\n",
    "    test_loss = 0\n",
    "    #with torch.no_grad():\n",
    "    for X, y in dataloader_2:\n",
    "        pred = model(X)\n",
    "        loss_result=loss(pred, y)\n",
    "        print(loss_result)\n",
    "        #print(\"test\",pred,y)\n",
    "        model.a.retain_grad()\n",
    "        model.b.retain_grad()\n",
    "        model.c.retain_grad()\n",
    "        loss_result.backward()\n",
    "    #test_loss /= num_batches\n",
    "    #print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    #print(model.test_on)\n",
    "    if model.test_on==1:\n",
    "        temp_sum=0\n",
    "        model.test_on=0\n",
    "        #print(\"악\")\n",
    "        #print(model.a.grad)\n",
    "        width_ = model.a.grad.shape[-1]\n",
    "        result_temp = torch.sum(model.a.grad.detach(),-1)\n",
    "        height_ = result_temp.shape[-1]\n",
    "        result_temp = torch.sum(result_temp,-1)\n",
    "        result_temp_ = torch.sum(result_temp,0)\n",
    "        result_temp = result_temp_/5\n",
    "        print(\"test\",result_temp)\n",
    "        model.count_=0\n",
    "        temp_sum=0\n",
    "        #print(result_temp.shape)\n",
    "        for k in range(1):  \n",
    "            for j in range(result_temp.shape[0]):\n",
    "                k=0\n",
    "                temp_grad = model.a.grad.detach().reshape((model.batch_size, 5, model.a.grad.shape[-2],model.a.grad.shape[-1]))\n",
    "                temp_sum += result_temp[j]*(temp_grad[0,j,:,:])\n",
    "            result_ = temp_sum.detach().cpu().numpy()\n",
    "            min_= np.min(result_)\n",
    "            result_=result_-min_\n",
    "            max_ = np.max(result_)\n",
    "            result_ = ((255*result_)/max_).astype(np.uint8)\n",
    "            \n",
    "            pil_image=Image.fromarray(result_)\n",
    "            #pil_name=str(k)+str(j)+\".png\"\n",
    "            result_=pil_image.resize((200,100))\n",
    "            pil_image.save(\"name_test.png\")\n",
    "    \n",
    "        temp_sum=0\n",
    "        width_ = model.b.grad.shape[-1]\n",
    "        result_temp = torch.sum(model.b.grad.detach(),-1)\n",
    "        height_ = result_temp.shape[-1]\n",
    "        result_temp = torch.sum(result_temp,-1)\n",
    "        result_temp_ = torch.sum(result_temp,0)\n",
    "        result_temp = result_temp_/5\n",
    "        model.count_=0\n",
    "        temp_sum=0\n",
    "        for k in range(1):  \n",
    "            for j in range(result_temp.shape[0]):\n",
    "                k=1\n",
    "                temp_grad = model.b.grad.detach().reshape((model.batch_size, 5, model.b.grad.shape[-2],model.b.grad.shape[-1]))\n",
    "                temp_sum += result_temp[j]*(temp_grad[0,j,:,:])\n",
    "            result_ = temp_sum.detach().cpu().numpy()\n",
    "            min_= np.min(result_)\n",
    "            result_=result_-min_\n",
    "            max_ = np.max(result_)\n",
    "            result_ = ((255*result_)/max_).astype(np.uint8)\n",
    "            #for t in range(96):\n",
    "                \n",
    "            #    print(result_[t,:])\n",
    "            pil_image=Image.fromarray(result_)\n",
    "            #pil_name=str(k)+str(j)+\".png\"\n",
    "            result_=pil_image.resize((200,100))\n",
    "            pil_image.save(\"name_middle_test.png\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader, model, loss, optimizer)\n",
    "    test(dataloader_test, model, loss, optimizer)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
